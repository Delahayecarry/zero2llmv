# MiniMindVLM ä½¿ç”¨æŒ‡å—ä¸APIæ–‡æ¡£

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒé…ç½®

```bash
# å®‰è£…ä¾èµ–
pip install torch torchvision transformers pillow

# æˆ–ä½¿ç”¨ uv (æ¨è)
uv add torch torchvision transformers pillow
```

### åŸºç¡€ä½¿ç”¨ç¤ºä¾‹

```python
import torch
from PIL import Image
from models.vision_encoder import MiniMindVLM, VLLMconfig

# 1. åˆå§‹åŒ–æ¨¡å‹é…ç½®
config = VLLMconfig(
    # åŸºç¡€è¯­è¨€æ¨¡å‹é…ç½®
    vocab_size=50000,
    hidden_size=768,
    num_attention_heads=12,
    num_hidden_layers=12,
    intermediate_size=3072,
    max_position_embeddings=2048,
    
    # å¤šæ¨¡æ€ç‰¹å®šé…ç½®
    image_special_token='@' * 196,  # å›¾åƒå ä½ç¬¦ (196ä¸ªpatch)
    image_ids=[34] * 196,           # å›¾åƒtokençš„IDåºåˆ—
    
    # é«˜çº§ç‰¹æ€§ (å¯é€‰)
    use_moe=False,                  # æ˜¯å¦å¯ç”¨MoE
    flash_attn=True,                # Flash AttentionåŠ é€Ÿ
    dropout=0.1,
    rope_theta=10000.0
)

# 2. åŠ è½½æ¨¡å‹
model = MiniMindVLM(
    params=config,
    vision_model_path="./models/clip-vit-base-patch16"  # CLIPæ¨¡å‹è·¯å¾„
)

# 3. å‡†å¤‡å¤šæ¨¡æ€è¾“å…¥
text = "è¯·æè¿°è¿™å¼ å›¾ç‰‡ä¸­çš„å†…å®¹ï¼š"
image = Image.open("example.jpg")

# 4. é¢„å¤„ç†
# å‡è®¾å·²æœ‰tokenizer
input_text = text + config.image_special_token  # æ·»åŠ å›¾åƒå ä½ç¬¦
input_ids = tokenizer(input_text, return_tensors='pt')['input_ids']

# å›¾åƒé¢„å¤„ç†
pixel_values = MiniMindVLM.image2tensor(image, model.processor)
pixel_values = pixel_values.unsqueeze(0)  # æ·»åŠ batchç»´åº¦

# 5. æ¨¡å‹æ¨ç†
with torch.no_grad():
    outputs = model(
        input_ids=input_ids,
        pixel_values=pixel_values,
        use_cache=True,  # å¯ç”¨KVç¼“å­˜åŠ é€Ÿ
        logits_to_keep=1  # åªä¿ç•™æœ€åä¸€ä¸ªtokençš„logits
    )

# 6. è·å–é¢„æµ‹ç»“æœ
logits = outputs.logits
probabilities = torch.softmax(logits[0, -1, :], dim=-1)
predicted_token_id = torch.argmax(probabilities).item()

print(f"é¢„æµ‹çš„ä¸‹ä¸€ä¸ªtoken ID: {predicted_token_id}")
```

## ğŸ”§ é«˜çº§é…ç½®ç¤ºä¾‹

### MoEå¤šä¸“å®¶æ¨¡å‹é…ç½®

```python
# å¤§è§„æ¨¡MoEé…ç½®
moe_config = VLLMconfig(
    # åŸºç¡€é…ç½®
    vocab_size=100000,
    hidden_size=1024,
    num_attention_heads=16,
    num_key_value_heads=8,  # GQA: å‡å°‘KVå¤´æ•°
    num_hidden_layers=24,
    
    # MoEä¸“å®¶æ··åˆé…ç½®
    use_moe=True,
    num_experts_per_token=2,    # æ¯ä¸ªtokenæ¿€æ´»2ä¸ªä¸“å®¶
    n_routed_experts=16,        # æ€»å…±16ä¸ªä¸“å®¶
    n_shared_experts=2,         # 2ä¸ªå…±äº«ä¸“å®¶
    aux_loss_alpha=0.01,        # è´Ÿè½½å‡è¡¡æŸå¤±æƒé‡
    norm_topk_prob=True,        # å½’ä¸€åŒ–ä¸“å®¶æƒé‡
    
    # æ€§èƒ½ä¼˜åŒ–
    flash_attn=True,
    use_cache=True,
    max_position_embeddings=4096
)

# åˆå§‹åŒ–MoEæ¨¡å‹
moe_model = MiniMindVLM(
    params=moe_config,
    vision_model_path="openai/clip-vit-large-patch14"
)

print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in moe_model.parameters()):,}")
print(f"æ¿€æ´»å‚æ•°é‡: {sum(p.numel() for p in moe_model.parameters() if p.requires_grad):,}")
```

### æ‰¹é‡æ¨ç†ç¤ºä¾‹

```python
def batch_multimodal_inference(model, texts, images, batch_size=4):
    """
    æ‰¹é‡å¤šæ¨¡æ€æ¨ç†å‡½æ•°
    
    Args:
        model: MiniMindVLMæ¨¡å‹å®ä¾‹
        texts: æ–‡æœ¬åˆ—è¡¨
        images: PILå›¾åƒåˆ—è¡¨  
        batch_size: æ‰¹å¤„ç†å¤§å°
        
    Returns:
        é¢„æµ‹ç»“æœåˆ—è¡¨
    """
    results = []
    
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i+batch_size]
        batch_images = images[i:i+batch_size]
        
        # é¢„å¤„ç†æ‰¹æ¬¡
        input_ids_list = []
        pixel_values_list = []
        
        for text, image in zip(batch_texts, batch_images):
            # æ–‡æœ¬å¤„ç†
            input_text = text + model.params.image_special_token
            input_ids = tokenizer(input_text, 
                                return_tensors='pt', 
                                padding=True, 
                                truncation=True)['input_ids']
            input_ids_list.append(input_ids)
            
            # å›¾åƒå¤„ç†
            pixel_values = MiniMindVLM.image2tensor(image, model.processor)
            pixel_values_list.append(pixel_values)
        
        # æ‰¹é‡å¼ é‡
        batch_input_ids = torch.cat(input_ids_list, dim=0)
        batch_pixel_values = torch.stack(pixel_values_list, dim=0)
        
        # æ‰¹é‡æ¨ç†
        with torch.no_grad():
            batch_outputs = model(
                input_ids=batch_input_ids,
                pixel_values=batch_pixel_values,
                use_cache=False  # æ‰¹é‡æ¨ç†æ—¶å…³é—­ç¼“å­˜
            )
        
        # å¤„ç†è¾“å‡º
        batch_logits = batch_outputs.logits
        batch_predictions = torch.argmax(batch_logits[:, -1, :], dim=-1)
        
        results.extend(batch_predictions.cpu().tolist())
    
    return results

# ä½¿ç”¨ç¤ºä¾‹
texts = ["æè¿°å›¾ç‰‡:", "è¿™æ˜¯ä»€ä¹ˆ:", "å›¾ç‰‡å†…å®¹:"]
images = [Image.open(f"image_{i}.jpg") for i in range(3)]

predictions = batch_multimodal_inference(model, texts, images)
```

## âš¡ ç”Ÿæˆå¼æ¨ç†ç¤ºä¾‹

### è‡ªå›å½’æ–‡æœ¬ç”Ÿæˆ

```python
def multimodal_generate(model, text_prompt, image, max_length=100, 
                       temperature=1.0, top_k=50, top_p=0.9):
    """
    å¤šæ¨¡æ€è‡ªå›å½’ç”Ÿæˆå‡½æ•°
    
    Args:
        model: MiniMindVLMæ¨¡å‹
        text_prompt: æ–‡æœ¬æç¤º
        image: PILå›¾åƒ
        max_length: ç”Ÿæˆçš„æœ€å¤§é•¿åº¦
        temperature: é‡‡æ ·æ¸©åº¦
        top_k: Top-Ké‡‡æ ·å‚æ•°
        top_p: Top-P (nucleus) é‡‡æ ·å‚æ•°
        
    Returns:
        generated_text: ç”Ÿæˆçš„æ–‡æœ¬
    """
    model.eval()
    device = next(model.parameters()).device
    
    # å‡†å¤‡åˆå§‹è¾“å…¥
    input_text = text_prompt + model.params.image_special_token
    input_ids = tokenizer(input_text, return_tensors='pt')['input_ids'].to(device)
    
    # å›¾åƒé¢„å¤„ç†
    pixel_values = MiniMindVLM.image2tensor(image, model.processor)
    pixel_values = pixel_values.unsqueeze(0).to(device)
    
    generated_tokens = []
    past_key_values = None
    
    with torch.no_grad():
        for step in range(max_length):
            # å½“å‰è¾“å…¥ (ç¬¬ä¸€æ­¥åŒ…å«å›¾åƒï¼Œåç»­æ­¥éª¤ä»…æ–‡æœ¬)
            current_pixel_values = pixel_values if step == 0 else None
            
            # æ¨¡å‹å‰å‘ä¼ æ’­
            outputs = model(
                input_ids=input_ids,
                pixel_values=current_pixel_values,
                past_key_values=past_key_values,
                use_cache=True
            )
            
            # è·å–ä¸‹ä¸€ä¸ªtokençš„logits
            next_token_logits = outputs.logits[0, -1, :] / temperature
            
            # Top-K + Top-P é‡‡æ ·
            if top_k > 0:
                top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)
                next_token_logits = torch.full_like(next_token_logits, float('-inf'))
                next_token_logits.scatter_(0, top_k_indices, top_k_logits)
            
            if top_p < 1.0:
                sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)
                cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)
                sorted_indices_to_remove = cumulative_probs > top_p
                sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()
                sorted_indices_to_remove[0] = 0
                indices_to_remove = sorted_indices[sorted_indices_to_remove]
                next_token_logits[indices_to_remove] = float('-inf')
            
            # é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
            probs = torch.softmax(next_token_logits, dim=-1)
            next_token = torch.multinomial(probs, 1)
            
            # æ£€æŸ¥ç»“æŸæ¡ä»¶
            if next_token.item() == tokenizer.eos_token_id:
                break
                
            generated_tokens.append(next_token.item())
            
            # å‡†å¤‡ä¸‹ä¸€æ­¥è¾“å…¥
            input_ids = next_token.unsqueeze(0)
            past_key_values = outputs.past_key_values
    
    # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
    generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)
    return generated_text

# ä½¿ç”¨ç¤ºä¾‹
image = Image.open("cat.jpg")
prompt = "è¿™å¼ å›¾ç‰‡æ˜¾ç¤ºäº†"

generated_description = multimodal_generate(
    model=model,
    text_prompt=prompt,
    image=image,
    max_length=50,
    temperature=0.7,
    top_k=40,
    top_p=0.9
)

print(f"ç”Ÿæˆçš„æè¿°: {prompt}{generated_description}")
```

## ğŸ§  è®­ç»ƒç›¸å…³API

### è®­ç»ƒå¾ªç¯ç¤ºä¾‹

```python
def train_multimodal_model(model, train_dataloader, optimizer, num_epochs):
    """
    å¤šæ¨¡æ€æ¨¡å‹è®­ç»ƒå¾ªç¯
    """
    model.train()
    device = next(model.parameters()).device
    
    for epoch in range(num_epochs):
        total_loss = 0
        total_aux_loss = 0
        
        for batch_idx, batch in enumerate(train_dataloader):
            # è·å–æ‰¹æ¬¡æ•°æ®
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            pixel_values = batch['pixel_values'].to(device) if 'pixel_values' in batch else None
            labels = batch['labels'].to(device)
            
            # å‰å‘ä¼ æ’­
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                pixel_values=pixel_values,
                labels=labels  # ä¼ å…¥labelsä¼šè‡ªåŠ¨è®¡ç®—äº¤å‰ç†µæŸå¤±
            )
            
            # è®¡ç®—æ€»æŸå¤±
            main_loss = outputs.loss
            aux_loss = outputs.aux_loss if hasattr(outputs, 'aux_loss') else 0
            total_loss_value = main_loss + aux_loss
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            total_loss_value.backward()
            
            # æ¢¯åº¦è£å‰ª (å¯é€‰)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            # ç»Ÿè®¡
            total_loss += main_loss.item()
            total_aux_loss += aux_loss if isinstance(aux_loss, float) else aux_loss.item()
            
            # æ—¥å¿—è¾“å‡º
            if batch_idx % 100 == 0:
                print(f'Epoch {epoch}, Batch {batch_idx}: '
                      f'Loss={main_loss.item():.4f}, '
                      f'Aux_Loss={aux_loss:.4f if isinstance(aux_loss, float) else aux_loss.item():.4f}')
        
        avg_loss = total_loss / len(train_dataloader)
        avg_aux_loss = total_aux_loss / len(train_dataloader)
        print(f'Epoch {epoch} completed: Avg_Loss={avg_loss:.4f}, Avg_Aux_Loss={avg_aux_loss:.4f}')

# ä¼˜åŒ–å™¨é…ç½®ç¤ºä¾‹
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR

# åˆ†å±‚å­¦ä¹ ç‡ï¼šè§†è§‰æŠ•å½±å™¨ä½¿ç”¨æ›´é«˜å­¦ä¹ ç‡
vision_params = []
language_params = []

for name, param in model.named_parameters():
    if 'vision_proj' in name:
        vision_params.append(param)
    else:
        language_params.append(param)

optimizer = AdamW([
    {'params': language_params, 'lr': 1e-4, 'weight_decay': 0.01},
    {'params': vision_params, 'lr': 5e-4, 'weight_decay': 0.01}  # è§†è§‰æŠ•å½±å™¨ä½¿ç”¨æ›´é«˜å­¦ä¹ ç‡
])

scheduler = CosineAnnealingLR(optimizer, T_max=1000, eta_min=1e-6)
```

## ğŸ“Š æ€§èƒ½åˆ†æå’Œç›‘æ§

```python
import time
import psutil
import torch.profiler

def benchmark_model_performance(model, sample_inputs, num_runs=100):
    """
    æ€§èƒ½åŸºå‡†æµ‹è¯•å‡½æ•°
    """
    model.eval()
    device = next(model.parameters()).device
    
    # é¢„çƒ­
    for _ in range(10):
        with torch.no_grad():
            _ = model(**sample_inputs)
    
    # åŒæ­¥GPU (å¦‚æœä½¿ç”¨CUDA)
    if device.type == 'cuda':
        torch.cuda.synchronize()
    
    # æ€§èƒ½æµ‹è¯•
    times = []
    memory_usage = []
    
    for i in range(num_runs):
        # è®°å½•å¼€å§‹æ—¶é—´å’Œå†…å­˜
        start_time = time.perf_counter()
        start_memory = torch.cuda.memory_allocated() if device.type == 'cuda' else psutil.virtual_memory().used
        
        # æ¨ç†
        with torch.no_grad():
            outputs = model(**sample_inputs)
        
        # åŒæ­¥å¹¶è®°å½•ç»“æŸæ—¶é—´å’Œå†…å­˜
        if device.type == 'cuda':
            torch.cuda.synchronize()
        end_time = time.perf_counter()
        end_memory = torch.cuda.memory_allocated() if device.type == 'cuda' else psutil.virtual_memory().used
        
        times.append(end_time - start_time)
        memory_usage.append(end_memory - start_memory)
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    import numpy as np
    times = np.array(times)
    memory_usage = np.array(memory_usage)
    
    print(f"æ€§èƒ½ç»Ÿè®¡ (åŸºäº {num_runs} æ¬¡è¿è¡Œ):")
    print(f"  å¹³å‡å»¶è¿Ÿ: {times.mean()*1000:.2f} Â± {times.std()*1000:.2f} ms")
    print(f"  æœ€å°å»¶è¿Ÿ: {times.min()*1000:.2f} ms")
    print(f"  æœ€å¤§å»¶è¿Ÿ: {times.max()*1000:.2f} ms")
    print(f"  å†…å­˜ä½¿ç”¨: {memory_usage.mean()/1024/1024:.2f} Â± {memory_usage.std()/1024/1024:.2f} MB")
    
    return {
        'latency_mean': times.mean(),
        'latency_std': times.std(),
        'memory_mean': memory_usage.mean(),
        'memory_std': memory_usage.std()
    }

# ä½¿ç”¨PyTorch Profilerè¿›è¡Œè¯¦ç»†åˆ†æ
def profile_model_execution(model, sample_inputs):
    """
    ä½¿ç”¨PyTorch Profileråˆ†ææ¨¡å‹æ‰§è¡Œ
    """
    with torch.profiler.profile(
        activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],
        record_shapes=True,
        profile_memory=True,
        with_stack=True
    ) as profiler:
        with torch.no_grad():
            outputs = model(**sample_inputs)
    
    # è¾“å‡ºprofilingç»“æœ
    print("Top 10 GPU operations by time:")
    print(profiler.key_averages().table(sort_by="cuda_time_total", row_limit=10))
    
    print("\nTop 10 CPU operations by time:")
    print(profiler.key_averages().table(sort_by="cpu_time_total", row_limit=10))
    
    # å¯¼å‡ºChrome traceæ–‡ä»¶
    profiler.export_chrome_trace("model_trace.json")
    print("Trace exported to model_trace.json")
    
    return profiler
```

## ğŸ”§ æ¨¡å‹é…ç½®è¯¦è§£

### å®Œæ•´é…ç½®é€‰é¡¹

```python
class VLLMconfig(llmconfig):
    """
    MiniMindVLMé…ç½®ç±» - æ‰€æœ‰å¯é…ç½®å‚æ•°è¯¦è§£
    """
    
    def __init__(
        self,
        # === åŸºç¡€è¯­è¨€æ¨¡å‹é…ç½® ===
        vocab_size: int = 30000,              # è¯æ±‡è¡¨å¤§å°
        hidden_size: int = 768,               # éšè—å±‚ç»´åº¦
        num_attention_heads: int = 12,        # æ³¨æ„åŠ›å¤´æ•°
        num_key_value_heads: Optional[int] = None,  # KVå¤´æ•° (GQA)
        num_hidden_layers: int = 12,          # Transformerå±‚æ•°
        intermediate_size: Optional[int] = None,    # FFNä¸­é—´å±‚ç»´åº¦
        max_position_embeddings: int = 2048,  # æœ€å¤§ä½ç½®ç¼–ç é•¿åº¦
        
        # === å¤šæ¨¡æ€ç‰¹å®šé…ç½® ===
        image_special_token: str = '@' * 196,       # å›¾åƒå ä½ç¬¦token
        image_ids: List[int] = [34] * 196,          # å›¾åƒtoken IDåºåˆ—
        ve_hidden_size: int = 768,                  # è§†è§‰ç¼–ç å™¨è¾“å‡ºç»´åº¦
        
        # === æ¿€æ´»å‡½æ•°å’Œæ­£åˆ™åŒ– ===
        hidden_act: str = "silu",             # æ¿€æ´»å‡½æ•°ç±»å‹
        dropout: float = 0.1,                 # Dropoutæ¦‚ç‡
        tie_word_embeddings: bool = True,     # æ˜¯å¦å…±äº«è¾“å…¥è¾“å‡ºåµŒå…¥
        
        # === ä½ç½®ç¼–ç é…ç½® ===
        rope_theta: float = 10000.0,          # RoPEåŸºæ•°
        
        # === MoEä¸“å®¶æ··åˆé…ç½® ===
        use_moe: bool = False,                # æ˜¯å¦å¯ç”¨MoE
        num_experts_per_token: int = 1,       # æ¯tokenæ¿€æ´»çš„ä¸“å®¶æ•°
        n_routed_experts: int = 2,            # è·¯ç”±ä¸“å®¶æ€»æ•°
        n_shared_experts: int = 0,            # å…±äº«ä¸“å®¶æ•°é‡
        aux_loss_alpha: float = 0.0,          # è¾…åŠ©æŸå¤±æƒé‡
        norm_topk_prob: bool = False,         # æ˜¯å¦å½’ä¸€åŒ–TopKæ¦‚ç‡
        seq_aux: bool = True,                 # åºåˆ—çº§è¾…åŠ©æŸå¤±
        
        # === æ€§èƒ½ä¼˜åŒ–é…ç½® ===
        flash_attn: bool = True,              # Flash Attention
        use_cache: bool = True,               # KVç¼“å­˜
        
        # === ç‰¹æ®Štokené…ç½® ===
        bos_token_id: int = 0,                # å¼€å§‹token ID
        eos_token_id: int = 1,                # ç»“æŸtoken ID
        pad_token_id: int = 2,                # å¡«å……token ID
        
        **kwargs
    ):
        # è®¾ç½®å›¾åƒç›¸å…³é…ç½®
        self.image_special_token = image_special_token
        self.image_ids = image_ids
        self.ve_hidden_size = ve_hidden_size
        
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().__init__(
            vocab_size=vocab_size,
            hidden_size=hidden_size,
            num_attention_heads=num_attention_heads,
            num_key_value_heads=num_key_value_heads,
            num_hidden_layers=num_hidden_layers,
            intermediate_size=intermediate_size,
            max_position_embeddings=max_position_embeddings,
            hidden_act=hidden_act,
            dropout=dropout,
            tie_word_embeddings=tie_word_embeddings,
            rope_theta=rope_theta,
            use_moe=use_moe,
            num_experts_per_token=num_experts_per_token,
            n_routed_experts=n_routed_experts,
            n_shared_experts=n_shared_experts,
            aux_loss_alpha=aux_loss_alpha,
            norm_topk_prob=norm_topk_prob,
            seq_aux=seq_aux,
            flash_attn=flash_attn,
            bos_token_id=bos_token_id,
            eos_token_id=eos_token_id,
            pad_token_id=pad_token_id,
            **kwargs
        )

# ä¸åŒè§„æ¨¡çš„é¢„è®¾é…ç½®
def get_model_configs():
    """è·å–ä¸åŒè§„æ¨¡çš„é¢„è®¾é…ç½®"""
    
    configs = {
        "tiny": VLLMconfig(
            vocab_size=10000,
            hidden_size=256,
            num_attention_heads=4,
            num_hidden_layers=6,
            intermediate_size=512,
            max_position_embeddings=1024
        ),
        
        "small": VLLMconfig(
            vocab_size=30000,
            hidden_size=768,
            num_attention_heads=12,
            num_hidden_layers=12,
            intermediate_size=3072,
            max_position_embeddings=2048
        ),
        
        "medium": VLLMconfig(
            vocab_size=50000,
            hidden_size=1024,
            num_attention_heads=16,
            num_key_value_heads=8,  # GQA
            num_hidden_layers=24,
            intermediate_size=4096,
            max_position_embeddings=4096
        ),
        
        "large_moe": VLLMconfig(
            vocab_size=100000,
            hidden_size=1536,
            num_attention_heads=24,
            num_key_value_heads=12,
            num_hidden_layers=32,
            use_moe=True,
            num_experts_per_token=2,
            n_routed_experts=16,
            n_shared_experts=2,
            aux_loss_alpha=0.01,
            max_position_embeddings=8192
        )
    }
    
    return configs

# ä½¿ç”¨é¢„è®¾é…ç½®
configs = get_model_configs()
small_model = MiniMindVLM(params=configs["small"])
```

## ğŸ” æ•…éšœæ’é™¤æŒ‡å—

### å¸¸è§é—®é¢˜è§£å†³

```python
def diagnose_model_issues(model, sample_input):
    """
    æ¨¡å‹é—®é¢˜è¯Šæ–­å·¥å…·
    """
    print("=== MiniMindVLM æ¨¡å‹è¯Šæ–­ ===")
    
    # 1. æ£€æŸ¥æ¨¡å‹é…ç½®
    config = model.params
    print(f"âœ“ æ¨¡å‹é…ç½®: {config.hidden_size}d, {config.num_hidden_layers}å±‚")
    print(f"âœ“ è¯æ±‡è¡¨å¤§å°: {config.vocab_size}")
    print(f"âœ“ MoEçŠ¶æ€: {'å¯ç”¨' if config.use_moe else 'ç¦ç”¨'}")
    
    # 2. æ£€æŸ¥è®¾å¤‡çŠ¶æ€
    device = next(model.parameters()).device
    print(f"âœ“ æ¨¡å‹è®¾å¤‡: {device}")
    
    # 3. æ£€æŸ¥è§†è§‰ç»„ä»¶
    if hasattr(model, 'vision_encoder') and model.vision_encoder is not None:
        print("âœ“ è§†è§‰ç¼–ç å™¨: å·²åŠ è½½")
        vision_params = sum(p.numel() for p in model.vision_encoder.parameters())
        print(f"  è§†è§‰ç¼–ç å™¨å‚æ•°é‡: {vision_params:,}")
    else:
        print("âš  è§†è§‰ç¼–ç å™¨: æœªåŠ è½½")
    
    # 4. æ£€æŸ¥å‚æ•°çŠ¶æ€
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"âœ“ æ€»å‚æ•°é‡: {total_params:,}")
    print(f"âœ“ å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    
    # 5. æµ‹è¯•å‰å‘ä¼ æ’­
    try:
        model.eval()
        with torch.no_grad():
            outputs = model(**sample_input)
        print("âœ“ å‰å‘ä¼ æ’­: æ­£å¸¸")
        print(f"  è¾“å‡ºå½¢çŠ¶: {outputs.logits.shape}")
        
        # æ£€æŸ¥è¾“å‡ºæ•°å€¼
        if torch.isnan(outputs.logits).any():
            print("âŒ è¾“å‡ºåŒ…å«NaNå€¼")
        elif torch.isinf(outputs.logits).any():
            print("âŒ è¾“å‡ºåŒ…å«Infå€¼")
        else:
            print("âœ“ è¾“å‡ºæ•°å€¼: æ­£å¸¸")
            
    except Exception as e:
        print(f"âŒ å‰å‘ä¼ æ’­å¤±è´¥: {str(e)}")
    
    # 6. å†…å­˜ä½¿ç”¨æ£€æŸ¥
    if device.type == 'cuda':
        memory_allocated = torch.cuda.memory_allocated() / 1024 / 1024
        memory_reserved = torch.cuda.memory_reserved() / 1024 / 1024
        print(f"âœ“ GPUå†…å­˜ä½¿ç”¨: {memory_allocated:.1f}MB / {memory_reserved:.1f}MB")
        
        if memory_allocated > memory_reserved * 0.9:
            print("âš  GPUå†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼Œå¯èƒ½å¯¼è‡´OOM")

# è¯Šæ–­å·¥å…·ä½¿ç”¨ç¤ºä¾‹
sample_input = {
    'input_ids': torch.randint(0, 1000, (1, 50)),
    'pixel_values': torch.randn(1, 1, 3, 224, 224)
}

diagnose_model_issues(model, sample_input)
```

---

## ğŸ“š APIå‚è€ƒ

### æ ¸å¿ƒç±»å’Œæ–¹æ³•

#### `VLLMconfig`
å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹é…ç½®ç±»

**å‚æ•°:**
- `image_special_token`: å›¾åƒå ä½ç¬¦å­—ç¬¦ä¸²
- `image_ids`: å›¾åƒtoken IDåˆ—è¡¨  
- å…¶ä»–å‚æ•°ç»§æ‰¿è‡ª`llmconfig`

#### `MiniMindVLM`
ä¸»è¦çš„å¤šæ¨¡æ€è§†è§‰è¯­è¨€æ¨¡å‹ç±»

**ä¸»è¦æ–¹æ³•:**

##### `__init__(params, vision_model_path)`
- `params`: VLLMconfigé…ç½®å¯¹è±¡
- `vision_model_path`: CLIPæ¨¡å‹è·¯å¾„

##### `forward(input_ids, pixel_values=None, attention_mask=None, ...)`
- `input_ids`: è¾“å…¥tokenåºåˆ— [B, L]
- `pixel_values`: å›¾åƒåƒç´ å€¼ [B, N, 3, 224, 224]
- `attention_mask`: æ³¨æ„åŠ›æ©ç  [B, L]
- `past_key_values`: KVç¼“å­˜
- `use_cache`: æ˜¯å¦å¯ç”¨KVç¼“å­˜
- `logits_to_keep`: ä¿ç•™çš„logitsæ•°é‡

**è¿”å›:** `ModelOutput`åŒ…å«logitsã€hidden_statesã€past_key_valuesç­‰

##### é™æ€æ–¹æ³•

- `get_vision_model(model_path)`: åŠ è½½CLIPæ¨¡å‹
- `image2tensor(image, processor)`: å›¾åƒè½¬å¼ é‡  
- `get_image_embeddings(image_tensors, vision_model)`: æå–å›¾åƒç‰¹å¾

#### `VisionEncoder`
è§†è§‰ç‰¹å¾æŠ•å½±å™¨

**å‚æ•°:**
- `ve_hidden_size`: è§†è§‰ç¼–ç å™¨è¾“å‡ºç»´åº¦
- `hidden_size`: ç›®æ ‡éšè—å±‚ç»´åº¦

è¿™ä»½æ–‡æ¡£æ¶µç›–äº†MiniMindVLMçš„æ ¸å¿ƒä½¿ç”¨æ–¹æ³•ã€é«˜çº§åŠŸèƒ½é…ç½®ã€æ€§èƒ½ä¼˜åŒ–æŠ€å·§å’Œæ•…éšœæ’é™¤æŒ‡å—ï¼Œä¸ºå¼€å‘è€…æä¾›äº†å®Œæ•´çš„ä½¿ç”¨å‚è€ƒã€‚