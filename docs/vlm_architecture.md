# VLM è§†è§‰è¯­è¨€æ¨¡å‹æ¶æ„æ–‡æ¡£

## ğŸ¯ æ¨¡å‹æ¦‚è§ˆ

VLMï¼ˆVision Language Modelï¼‰æ˜¯åŸºäºå¼ºå¤§çš„ CausalLM å¤§è¯­è¨€æ¨¡å‹æ¶æ„æ‰©å±•çš„å¤šæ¨¡æ€è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤ŸåŒæ—¶ç†è§£å’Œå¤„ç†æ–‡æœ¬ä¸å›¾åƒä¿¡æ¯ã€‚

### æ ¸å¿ƒç‰¹æ€§
- ğŸ§  **ç»Ÿä¸€æ¶æ„**: åŸºäº Transformer çš„ç«¯åˆ°ç«¯å¤šæ¨¡æ€å­¦ä¹ 
- ğŸ”„ **è§†è§‰-è¯­è¨€å¯¹é½**: é€šè¿‡æŠ•å½±å±‚å®ç°è§†è§‰ä¸æ–‡æœ¬ç‰¹å¾çš„è¯­ä¹‰å¯¹é½  
- âš¡ **é«˜æ•ˆæ¨ç†**: æ”¯æŒ KV ç¼“å­˜åŠ é€Ÿæ–‡æœ¬ç”Ÿæˆ
- ğŸ¯ **çµæ´»æ‰©å±•**: æ”¯æŒä¸åŒè§„æ¨¡çš„è§†è§‰ç¼–ç å™¨å’Œè¯­è¨€æ¨¡å‹ç»„åˆ
- ğŸ”§ **MoE æ”¯æŒ**: ç»§æ‰¿ CausalLM çš„ä¸“å®¶æ··åˆæ¶æ„ä¼˜åŠ¿

## ğŸ—ï¸ æ•´ä½“æ¶æ„æµç¨‹å›¾

```mermaid
flowchart TD
    %% è¾“å…¥å¤„ç†é˜¶æ®µ
    subgraph Input["ğŸ“¥ å¤šæ¨¡æ€è¾“å…¥å¤„ç†"]
        A1["æ–‡æœ¬è¾“å…¥<br/>ğŸ“ input_ids<br/>ğŸ”¢ [batch_size, seq_len]"]
        A2["å›¾åƒè¾“å…¥<br/>ğŸ–¼ï¸ pixel_values<br/>ğŸ“ [batch_size, num_imgs, 3, 224, 224]"]
    end
    
    %% ç¼–ç é˜¶æ®µ
    subgraph Encoding["ğŸ”„ ç‰¹å¾ç¼–ç é˜¶æ®µ"]
        B1["æ–‡æœ¬åµŒå…¥<br/>ğŸ“š Embedding Layer<br/>ğŸ“Š [batch_size, seq_len, hidden_size]"]
        B2["CLIP è§†è§‰ç¼–ç å™¨<br/>ğŸ‘ï¸ Vision Transformer<br/>ğŸ”’ å†»ç»“å‚æ•°"]
        B3["å›¾åƒç‰¹å¾æå–<br/>ğŸ¯ Patch Features<br/>ğŸ“ˆ [batch_size, 196, clip_dim]"]
        
        A1 --> B1
        A2 --> B2
        B2 --> B3
    end
    
    %% ç‰¹å¾å¯¹é½é˜¶æ®µ  
    subgraph Alignment["ğŸ¯ è·¨æ¨¡æ€ç‰¹å¾å¯¹é½"]
        C1["è§†è§‰æŠ•å½±å™¨<br/>ğŸ”„ Linear Projection<br/>ğŸ“ clip_dim â†’ hidden_size"]
        C2["å›¾åƒå ä½ç¬¦å®šä½<br/>ğŸ” Image Token Matching<br/>ğŸ¯ æ‰¾åˆ° '@' * 196 åºåˆ—"]
        C3["ç‰¹å¾èåˆ<br/>ğŸ”— Vision-Text Fusion<br/>ğŸ’¡ æ›¿æ¢å ä½ç¬¦ä¸ºè§†è§‰ç‰¹å¾"]
        
        B3 --> C1
        B1 --> C2
        C1 --> C3
        C2 --> C3
    end
    
    %% Transformerå¤„ç†é˜¶æ®µ
    subgraph Transformer["ğŸ§  ç»Ÿä¸€Transformerå¤„ç†"]
        D1["å¤šæ¨¡æ€åºåˆ—<br/>ğŸ”„ Mixed Sequence<br/>ğŸ“Š [batch_size, seq_len, hidden_size]"]
        D2["ä½ç½®ç¼–ç <br/>ğŸ“ RoPE Encoding<br/>ğŸŒ€ æ—‹è½¬ä½ç½®åµŒå…¥"]
        D3["å¤šå±‚Transformer<br/>âš¡ N Ã— TransformerBlock<br/>ğŸ¯ æ³¨æ„åŠ› + FFN/MoE"]
        D4["å±‚å½’ä¸€åŒ–<br/>ğŸ“ Final RMSNorm<br/>ğŸ¯ è¾“å‡ºæ ‡å‡†åŒ–"]
        
        C3 --> D1
        D1 --> D2
        D2 --> D3
        D3 --> D4
    end
    
    %% è¾“å‡ºç”Ÿæˆé˜¶æ®µ
    subgraph Output["ğŸ“¤ è¾“å‡ºç”Ÿæˆ"]
        E1["è¯­è¨€å»ºæ¨¡å¤´<br/>ğŸ¯ LM Head<br/>ğŸ“Š [batch_size, seq_len, vocab_size]"]
        E2["æ¦‚ç‡åˆ†å¸ƒ<br/>ğŸ“ˆ Token Probabilities<br/>ğŸ² ä¸‹ä¸€ä¸ªtokené¢„æµ‹"]
        
        D4 --> E1
        E1 --> E2
    end
    
    %% KVç¼“å­˜æ”¯æŒ
    subgraph Cache["ğŸ’¾ KVç¼“å­˜æœºåˆ¶"]
        F1["ç¼“å­˜å­˜å‚¨<br/>ğŸ’½ Past Key-Values<br/>âš¡ åŠ é€Ÿç”Ÿæˆæ¨ç†"]
        F2["ç¼“å­˜æ›´æ–°<br/>ğŸ”„ Cache Update<br/>ğŸ¯ å¢é‡è®¡ç®—"]
        
        D3 -.-> F1
        F1 -.-> F2
        F2 -.-> D3
    end
    
    %% æ ·å¼å®šä¹‰
    classDef inputStyle fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef encodingStyle fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px  
    classDef alignmentStyle fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef transformerStyle fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef outputStyle fill:#fce4ec,stroke:#c2185b,stroke-width:2px
    classDef cacheStyle fill:#f1f8e9,stroke:#689f38,stroke-width:2px
    
    class A1,A2 inputStyle
    class B1,B2,B3 encodingStyle
    class C1,C2,C3 alignmentStyle
    class D1,D2,D3,D4 transformerStyle
    class E1,E2 outputStyle
    class F1,F2 cacheStyle
```

## ğŸ” è§†è§‰å¤„ç†è¯¦ç»†æµç¨‹

```mermaid
flowchart TD
    subgraph VisionPipeline["ğŸ‘ï¸ è§†è§‰å¤„ç†ç®¡é“è¯¦è§£"]
        direction TB
        
        %% å›¾åƒé¢„å¤„ç†
        V1["åŸå§‹å›¾åƒè¾“å…¥<br/>ğŸ–¼ï¸ PIL Image<br/>ğŸ“ ä»»æ„å°ºå¯¸"]
        V2["å›¾åƒæ ‡å‡†åŒ–<br/>ğŸ”„ Format Conversion<br/>ğŸ“ ç¡®ä¿RGBæ ¼å¼"]
        V3["CLIPé¢„å¤„ç†<br/>âš™ï¸ CLIPProcessor<br/>ğŸ“ Resize + Normalize"]
        V4["å¼ é‡è½¬æ¢<br/>ğŸ”¢ Tensor Format<br/>ğŸ“Š [1, 3, 224, 224]"]
        
        %% CLIPç¼–ç 
        V5["ViTç¼–ç å™¨<br/>ğŸ§  Vision Transformer<br/>ğŸ”’ å†»ç»“é¢„è®­ç»ƒå‚æ•°"]
        V6["Patchç‰¹å¾<br/>ğŸ“Š Patch Embeddings<br/>ğŸ“ [1, 197, 768]"]
        V7["ç§»é™¤CLS token<br/>âœ‚ï¸ Remove [CLS]<br/>ğŸ“‰ [1, 196, 768]"]
        V8["ç‰¹å¾æå–å®Œæˆ<br/>âœ… Vision Features<br/>ğŸ¯ 196ä¸ªpatchç‰¹å¾"]
        
        V1 --> V2
        V2 --> V3  
        V3 --> V4
        V4 --> V5
        V5 --> V6
        V6 --> V7
        V7 --> V8
        
        %% å¤„ç†ç»†èŠ‚æ³¨é‡Š
        V2 -.-> |"RGBA/LA â†’ RGB"| V3
        V5 -.-> |"14Ã—14 patches"| V6
        V6 -.-> |"197 = 1[CLS] + 196patches"| V7
        V7 -.-> |"åªä¿ç•™patchç‰¹å¾"| V8
    end
    
    subgraph Projection["ğŸ¯ ç‰¹å¾æŠ•å½±ä¸å¯¹é½"]
        direction TB
        
        P1["è§†è§‰ç‰¹å¾<br/>ğŸ“Š [196, 768]<br/>ğŸ” CLIPè¾“å‡ºç»´åº¦"]
        P2["çº¿æ€§æŠ•å½±<br/>ğŸ”„ Linear Layer<br/>ğŸ“ 768 â†’ hidden_size"]
        P3["ç»´åº¦å¯¹é½<br/>âœ… Aligned Features<br/>ğŸ“Š [196, hidden_size]"]
        P4["å‡†å¤‡èåˆ<br/>ğŸ”— Ready for Fusion<br/>ğŸ¯ ä¸æ–‡æœ¬ç‰¹å¾å…¼å®¹"]
        
        P1 --> P2
        P2 --> P3
        P3 --> P4
        
        V8 --> P1
    end
```

## ğŸ”— å¤šæ¨¡æ€ç‰¹å¾èåˆæœºåˆ¶

```mermaid
flowchart TD
    subgraph Fusion["ğŸ”— è§†è§‰-æ–‡æœ¬ç‰¹å¾èåˆè¯¦è§£"]
        direction TB
        
        %% è¾“å…¥å‡†å¤‡
        F1["æ–‡æœ¬åºåˆ—<br/>ğŸ“ Text Tokens<br/>ğŸ“Š [batch_size, seq_len]"]
        F2["è§†è§‰ç‰¹å¾<br/>ğŸ‘ï¸ Vision Features<br/>ğŸ“Š [batch_size, 196, hidden_size]"]
        F3["å›¾åƒå ä½ç¬¦<br/>ğŸ¯ Image Placeholders<br/>ğŸ”¤ [@@@...@@@] Ã— 196"]
        
        %% ä½ç½®åŒ¹é…
        F4["å ä½ç¬¦æœç´¢<br/>ğŸ” Token Matching<br/>âš™ï¸ æ»‘åŠ¨çª—å£ç®—æ³•"]
        F5["ä½ç½®ç´¢å¼•<br/>ğŸ“ Position Indices<br/>ğŸ“‹ [(start_idx, end_idx), ...]"]
        
        %% ç‰¹å¾æ›¿æ¢  
        F6["åºåˆ—é‡æ„<br/>ğŸ”„ Sequence Reconstruction<br/>ğŸ¯ é€batchå¤„ç†"]
        F7["ç‰¹å¾æ‹¼æ¥<br/>ğŸ§© Feature Concatenation<br/>ğŸ”— text_prefix + vision + text_suffix"]
        F8["èåˆåºåˆ—<br/>âœ… Fused Sequence<br/>ğŸ“Š [batch_size, seq_len, hidden_size]"]
        
        F1 --> F4
        F3 --> F4
        F4 --> F5
        F2 --> F6
        F5 --> F6
        F6 --> F7
        F7 --> F8
        
        %% ç®—æ³•ç»†èŠ‚
        subgraph Algorithm["ğŸ§® èåˆç®—æ³•è¯¦è§£"]
            A1["1ï¸âƒ£ éå†æ¯ä¸ªbatch"]
            A2["2ï¸âƒ£ å®šä½å›¾åƒå ä½ç¬¦ä½ç½®"]
            A3["3ï¸âƒ£ æå–å¯¹åº”è§†è§‰ç‰¹å¾"]
            A4["4ï¸âƒ£ æ‰§è¡Œå¼ é‡æ‹¼æ¥æ“ä½œ"]
            A5["5ï¸âƒ£ æˆªæ–­åˆ°æœ€å¤§åºåˆ—é•¿åº¦"]
            
            A1 --> A2 --> A3 --> A4 --> A5
        end
        
        F6 -.-> Algorithm
    end
    
    subgraph Example["ğŸ“ èåˆç¤ºä¾‹"]
        direction LR
        
        EX1["è¾“å…¥: [ä½ å¥½, @, @, ..., @, ä¸–ç•Œ]<br/>ğŸ“ seq_len = 200"]
        EX2["è§†è§‰: [v1, v2, ..., v196]<br/>ğŸ“Š 196ä¸ªpatchç‰¹å¾"]  
        EX3["è¾“å‡º: [ä½ å¥½, v1, v2, ..., v196, ä¸–ç•Œ]<br/>ğŸ”— èåˆåçš„å¤šæ¨¡æ€åºåˆ—"]
        
        EX1 --> EX2 --> EX3
    end
```

## âš™ï¸ Transformerå¤„ç†æµç¨‹

```mermaid
flowchart TD
    subgraph TransformerFlow["ğŸ§  Transformerå¤šæ¨¡æ€å¤„ç†æµç¨‹"]
        direction TB
        
        %% è¾“å…¥å¤„ç†
        T1["èåˆåºåˆ—è¾“å…¥<br/>ğŸ”— Multimodal Sequence<br/>ğŸ“Š [batch_size, seq_len, hidden_size]"]
        T2["RoPEä½ç½®ç¼–ç <br/>ğŸ“ Rotary Position Embedding<br/>ğŸŒ€ cos/sinä½ç½®ä¿¡æ¯"]
        
        %% å¤šå±‚å¤„ç†
        T3["TransformerBlock 0<br/>ğŸ¯ Attention + FFN/MoE<br/>âš¡ è‡ªæ³¨æ„åŠ›æœºåˆ¶"]
        T4["TransformerBlock 1<br/>ğŸ¯ Attention + FFN/MoE<br/>âš¡ è·¨æ¨¡æ€ä¿¡æ¯äº¤äº’"] 
        T5["TransformerBlock ...<br/>ğŸ¯ Attention + FFN/MoE<br/>âš¡ æ·±å±‚è¯­ä¹‰ç†è§£"]
        T6["TransformerBlock N-1<br/>ğŸ¯ Attention + FFN/MoE<br/>âš¡ æœ€ç»ˆç‰¹å¾æå–"]
        
        %% è¾“å‡ºå¤„ç†
        T7["æœ€ç»ˆRMSNorm<br/>ğŸ“ Output Normalization<br/>ğŸ¯ ç‰¹å¾æ ‡å‡†åŒ–"]
        T8["è¯­è¨€å»ºæ¨¡å¤´<br/>ğŸ¯ LM Head<br/>ğŸ“Š è¯æ±‡è¡¨æ¦‚ç‡åˆ†å¸ƒ"]
        
        T1 --> T2
        T2 --> T3
        T3 --> T4
        T4 --> T5  
        T5 --> T6
        T6 --> T7
        T7 --> T8
        
        %% æ³¨æ„åŠ›æœºåˆ¶è¯¦è§£
        subgraph Attention["ğŸ‘ï¸ è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶"]
            AT1["ğŸ” è§†è§‰-è§†è§‰æ³¨æ„åŠ›<br/>patché—´çš„ç©ºé—´å…³ç³»å»ºæ¨¡"]
            AT2["ğŸ“ æ–‡æœ¬-æ–‡æœ¬æ³¨æ„åŠ›<br/>è¯æ±‡é—´çš„è¯­ä¹‰å…³ç³»"]
            AT3["ğŸ”— è§†è§‰-æ–‡æœ¬æ³¨æ„åŠ›<br/>è·¨æ¨¡æ€è¯­ä¹‰å¯¹é½"]
            AT4["ğŸ§  ç»Ÿä¸€è¡¨ç¤ºå­¦ä¹ <br/>å¤šæ¨¡æ€è¯­ä¹‰èåˆ"]
            
            AT1 --> AT4
            AT2 --> AT4  
            AT3 --> AT4
        end
        
        T3 -.-> Attention
        T4 -.-> Attention
        T5 -.-> Attention
        T6 -.-> Attention
        
        %% KVç¼“å­˜æœºåˆ¶
        subgraph KVCache["ğŸ’¾ KVç¼“å­˜ä¼˜åŒ–"]
            KC1["åˆå§‹ç¼–ç <br/>ğŸ’½ Full Context Processing<br/>ğŸ¯ å¤„ç†å®Œæ•´å¤šæ¨¡æ€åºåˆ—"]
            KC2["å¢é‡ç”Ÿæˆ<br/>âš¡ Incremental Generation<br/>ğŸ”„ åªè®¡ç®—æ–°token"]
            KC3["ç¼“å­˜æ›´æ–°<br/>ğŸ”„ Cache Update<br/>ğŸ“ˆ ç´¯ç§¯å†å²ä¸Šä¸‹æ–‡"]
            
            KC1 --> KC2 --> KC3 --> KC2
        end
        
        T3 -.-> KVCache
        T4 -.-> KVCache
        T5 -.-> KVCache
        T6 -.-> KVCache
    end
```

## ğŸ“Š å¼ é‡ç»´åº¦å˜æ¢è¯¦è§£

```mermaid
flowchart LR
    subgraph TensorFlow["ğŸ“ å¼ é‡æµè½¬æ¢è¯¦ç»†è¿½è¸ª"]
        direction TB
        
        %% è¾“å…¥é˜¶æ®µ
        subgraph Input["ğŸ“¥ è¾“å…¥å¼ é‡"]
            I1["input_ids<br/>ğŸ“Š [B, L]<br/>ğŸ”¢ int64"]
            I2["pixel_values<br/>ğŸ“Š [B, N, 3, 224, 224]<br/>ğŸ”¢ float32"]
        end
        
        %% ç¼–ç é˜¶æ®µ
        subgraph Encode["ğŸ”„ ç¼–ç é˜¶æ®µ"]
            E1["text_embeds<br/>ğŸ“Š [B, L, H]<br/>ğŸ’¡ æ–‡æœ¬åµŒå…¥"]
            E2["vision_features<br/>ğŸ“Š [B, N, 196, C]<br/>ğŸ‘ï¸ CLIPç‰¹å¾"]
            E3["vision_proj<br/>ğŸ“Š [B, N, 196, H]<br/>ğŸ¯ æŠ•å½±ç‰¹å¾"]
        end
        
        %% èåˆé˜¶æ®µ
        subgraph Fusion["ğŸ”— ç‰¹å¾èåˆ"]
            F1["multimodal_seq<br/>ğŸ“Š [B, L, H]<br/>ğŸ”— èåˆåºåˆ—"]
        end
        
        %% Transformeré˜¶æ®µ
        subgraph Transform["ğŸ§  Transformer"]
            TR1["hidden_states<br/>ğŸ“Š [B, L, H]<br/>âš¡ æ¯å±‚è¾“å‡º"]
            TR2["final_hidden<br/>ğŸ“Š [B, L, H]<br/>ğŸ¯ æœ€ç»ˆéšè—æ€"]
        end
        
        %% è¾“å‡ºé˜¶æ®µ
        subgraph Output["ğŸ“¤ æœ€ç»ˆè¾“å‡º"]  
            O1["logits<br/>ğŸ“Š [B, L, V]<br/>ğŸ“ˆ è¯æ±‡æ¦‚ç‡"]
        end
        
        %% å¼ é‡æµè½¬æ¢
        I1 --> E1
        I2 --> E2
        E2 --> E3
        E1 --> F1
        E3 --> F1
        F1 --> TR1
        TR1 --> TR2
        TR2 --> O1
        
        %% ç»´åº¦è¯´æ˜
        subgraph Legend["ğŸ“ ç»´åº¦è¯´æ˜"]
            L1["B = batch_size (æ‰¹æ¬¡å¤§å°)"]
            L2["L = seq_len (åºåˆ—é•¿åº¦)"]
            L3["H = hidden_size (éšè—ç»´åº¦)"]
            L4["N = num_images (å›¾åƒæ•°é‡)"]
            L5["C = clip_hidden_size (CLIPç»´åº¦)"]
            L6["V = vocab_size (è¯æ±‡è¡¨å¤§å°)"]
        end
    end
```

## ğŸ¯ æ ¸å¿ƒç»„ä»¶ç±»å›¾

```mermaid
classDiagram
    %% é…ç½®ç±»
    class VLLMconfig {
        +str model_type
        +str image_special_token
        +List image_ids
        +__init__(image_special_token, image_ids, **kwargs)
    }
    
    %% è§†è§‰æŠ•å½±å™¨
    class VisionEncoder {
        +int ve_hidden_size
        +int hidden_size  
        +nn.Sequential vision_proj
        +__init__(ve_hidden_size, hidden_size)
        +forward(image_encoders) torch.Tensor
    }
    
    %% ä¸»è¦çš„VLMæ¨¡å‹
    class VLM {
        +VLLMconfig params
        +CLIPModel vision_encoder
        +CLIPProcessor processor
        +VisionEncoder vision_proj
        +__init__(params, vision_model_path)
        +get_vision_model(model_path)$ Tuple
        +image2tensor(image, processor)$ torch.Tensor
        +get_image_embeddings(image_tensors, vision_model)$ torch.Tensor
        +count_vision_proj(tokens, h, vision_tensors, seqlen) torch.Tensor
        +forward(input_ids, attention_mask, past_key_values, use_cache, logits_to_keep, pixel_values) ModelOutput
    }
    
    %% åŸºç¡€è¯­è¨€æ¨¡å‹  
    class CausalLM {
        <<abstract>>
        +LLM model
        +nn.Linear lm_head
        +forward() ModelOutput
    }
    
    %% HuggingFaceç»„ä»¶
    class CLIPModel {
        <<external>>
        +vision_model
        +text_model
    }
    
    class CLIPProcessor {
        <<external>>
        +process(images, return_tensors)
    }
    
    %% ç»§æ‰¿å…³ç³»
    llmconfig <|-- VLLMconfig
    CausalLM <|-- VLM
    torch_nn_Module <|-- VisionEncoder
    
    %% ç»„åˆå…³ç³»
    VLM *-- VLLMconfig
    VLM *-- VisionEncoder
    VLM *-- CLIPModel
    VLM *-- CLIPProcessor
    
    %% ä¾èµ–å…³ç³»
    VisionEncoder ..> torch_nn_Module : uses
    VLM ..> torch : uses
```

## ğŸš€ æ¨ç†ç”Ÿæˆæµç¨‹

```mermaid
sequenceDiagram
    participant User as ğŸ‘¤ ç”¨æˆ·
    participant Model as ğŸ§  VLM
    participant Vision as ğŸ‘ï¸ CLIPç¼–ç å™¨
    participant Proj as ğŸ¯ è§†è§‰æŠ•å½±å™¨
    participant Trans as âš¡ Transformer
    participant Cache as ğŸ’¾ KVç¼“å­˜
    
    Note over User,Cache: å¤šæ¨¡æ€æ¨ç†ç”Ÿæˆæ—¶åºå›¾
    
    %% åˆå§‹è¾“å…¥
    User->>Model: ğŸ“¤ å›¾æ–‡è¾“å…¥ (text + images)
    Model->>Vision: ğŸ–¼ï¸ å¤„ç†å›¾åƒ pixel_values
    Vision-->>Model: ğŸ“Š è¿”å›è§†è§‰ç‰¹å¾ [B,196,768]
    
    %% ç‰¹å¾å¤„ç†
    Model->>Proj: ğŸ”„ æŠ•å½±è§†è§‰ç‰¹å¾
    Proj-->>Model: âœ… å¯¹é½ç‰¹å¾ [B,196,H]
    
    %% ç‰¹å¾èåˆ
    Model->>Model: ğŸ”— æ–‡æœ¬-è§†è§‰èåˆ
    Note over Model: æ›¿æ¢å›¾åƒå ä½ç¬¦ä¸ºè§†è§‰ç‰¹å¾
    
    %% ç¼–ç é˜¶æ®µ
    Model->>Trans: ğŸ§  Transformerå¤„ç†
    Trans->>Cache: ğŸ’½ ç”ŸæˆKVç¼“å­˜
    Trans-->>Model: ğŸ“Š è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒ
    Cache-->>Model: ğŸ’¾ è¿”å›ç¼“å­˜çŠ¶æ€
    
    %% ç”Ÿæˆå¾ªç¯
    loop é€æ­¥ç”Ÿæˆ
        Model->>User: ğŸ“ è¾“å‡ºå½“å‰token
        User->>Model: ğŸ”„ ç»§ç»­ç”Ÿæˆè¯·æ±‚
        Model->>Trans: âš¡ å¢é‡è®¡ç®— (ä»…æ–°token)
        Trans->>Cache: ğŸ”„ æ›´æ–°KVç¼“å­˜
        Cache-->>Trans: ğŸ’½ å†å²ä¸Šä¸‹æ–‡
        Trans-->>Model: ğŸ“ˆ æ–°tokenæ¦‚ç‡
    end
    
    Model->>User: âœ… ç”Ÿæˆå®Œæˆ
```

## ğŸ›ï¸ é…ç½®å‚æ•°è¯´æ˜

```mermaid
mindmap
  root((ğŸ›ï¸ VLM<br/>é…ç½®å‚æ•°))
    (ğŸ§  è¯­è¨€æ¨¡å‹é…ç½®)
      hidden_size
      num_attention_heads
      num_hidden_layers
      vocab_size
      max_position_embeddings
    (ğŸ‘ï¸ è§†è§‰é…ç½®)
      image_special_token
      image_ids
      vision_model_path
      ve_hidden_size
    (âš¡ æ€§èƒ½ä¼˜åŒ–)
      use_cache
      flash_attn
      dropout
    (ğŸ”§ MoEé…ç½®)
      use_moe
      num_experts_per_token
      n_routed_experts
      aux_loss_alpha
    (ğŸ¯ è®­ç»ƒé…ç½®)
      tie_word_embeddings
      rope_theta
      logits_to_keep
```

## ğŸ“ˆ æ€§èƒ½ç‰¹æ€§å¯¹æ¯”

```mermaid
graph TD
    subgraph Comparison["ğŸ“Š æ¨¡å‹æ¶æ„å¯¹æ¯”"]
        direction TB
        
        subgraph Traditional["ğŸ—ï¸ ä¼ ç»Ÿå¤šæ¨¡æ€æ–¹æ¡ˆ"]
            T1["ç‹¬ç«‹è§†è§‰ç¼–ç å™¨<br/>ğŸ‘ï¸ Separate Vision Model"]
            T2["ç‹¬ç«‹è¯­è¨€æ¨¡å‹<br/>ğŸ“ Separate Language Model"] 
            T3["åæœŸç‰¹å¾èåˆ<br/>ğŸ”— Late Fusion"]
            T4["å¤šé˜¶æ®µè®­ç»ƒ<br/>ğŸ“š Multi-stage Training"]
            
            T1 --> T3
            T2 --> T3
            T3 --> T4
        end
        
        subgraph VLMSolution["ğŸš€ VLMæ–¹æ¡ˆ"]
            M1["ç»Ÿä¸€Transformeræ¶æ„<br/>ğŸ§  Unified Architecture"]
            M2["ç«¯åˆ°ç«¯è®­ç»ƒ<br/>âš¡ End-to-end Training"]
            M3["æ—©æœŸç‰¹å¾èåˆ<br/>ğŸ”— Early Fusion"]
            M4["KVç¼“å­˜ä¼˜åŒ–<br/>ğŸ’¾ Efficient Generation"]
            
            M1 --> M2
            M2 --> M3
            M3 --> M4
        end
        
        %% æ€§èƒ½å¯¹æ¯”
        subgraph Performance["âš¡ æ€§èƒ½ä¼˜åŠ¿"]
            P1["ğŸš€ æ›´å¿«çš„æ¨ç†é€Ÿåº¦<br/>ç»Ÿä¸€æ¶æ„å‡å°‘è®¡ç®—å¼€é”€"]
            P2["ğŸ’¾ æ›´é«˜çš„å†…å­˜æ•ˆç‡<br/>KVç¼“å­˜+å‚æ•°å…±äº«"]
            P3["ğŸ¯ æ›´å¥½çš„å¯¹é½æ•ˆæœ<br/>ç«¯åˆ°ç«¯è”åˆä¼˜åŒ–"]
            P4["ğŸ”§ æ›´å¼ºçš„å¯æ‰©å±•æ€§<br/>æ”¯æŒMoEç­‰é«˜çº§ç‰¹æ€§"]
        end
    end
```

## ğŸ”§ ä½¿ç”¨ç¤ºä¾‹ä»£ç 

```python
# æ¨¡å‹åˆå§‹åŒ–
from models.vision_encoder import VLM, VLLMconfig

# é…ç½®å¤šæ¨¡æ€æ¨¡å‹å‚æ•°
config = VLLMconfig(
    hidden_size=768,
    num_attention_heads=12,
    num_hidden_layers=12,
    vocab_size=50000,
    image_special_token='@' * 196,  # 196ä¸ªpatchå¯¹åº”çš„å ä½ç¬¦
    image_ids=[34] * 196,           # å›¾åƒtokençš„IDåºåˆ—
    use_moe=True,                   # å¯ç”¨MoEæ¶æ„
    num_experts_per_token=2,        # æ¯tokené€‰æ‹©2ä¸ªä¸“å®¶
    n_routed_experts=8              # æ€»å…±8ä¸ªä¸“å®¶
)

# åŠ è½½æ¨¡å‹
model = VLM(
    params=config,
    vision_model_path="./models/clip-vit-base-patch16"
)

# æ¨ç†ç¤ºä¾‹
import torch
from PIL import Image

# å‡†å¤‡è¾“å…¥
text = "è¯·æè¿°è¿™å¼ å›¾ç‰‡ï¼š"
image = Image.open("example.jpg")

# æ–‡æœ¬tokenization (å‡è®¾å·²æœ‰tokenizer)
input_ids = tokenizer(text + '@' * 196, return_tensors='pt')['input_ids']

# å›¾åƒé¢„å¤„ç†
pixel_values = VLM.image2tensor(image, model.processor).unsqueeze(0)

# æ¨¡å‹æ¨ç†
with torch.no_grad():
    outputs = model(
        input_ids=input_ids,
        pixel_values=pixel_values,
        use_cache=True  # å¯ç”¨KVç¼“å­˜åŠ é€Ÿç”Ÿæˆ
    )
    
# è·å–é¢„æµ‹ç»“æœ
logits = outputs.logits
predicted_token_id = torch.argmax(logits[0, -1, :]).item()
```

---

## ğŸ“ æ€»ç»“

VLM é€šè¿‡ä»¥ä¸‹æ ¸å¿ƒè®¾è®¡å®ç°äº†é«˜æ•ˆçš„å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆï¼š

1. **ğŸ”— æ—©æœŸç‰¹å¾èåˆ**: åœ¨Transformerå¤„ç†å‰å°±å®Œæˆè§†è§‰-æ–‡æœ¬ç‰¹å¾å¯¹é½
2. **ğŸ§  ç»Ÿä¸€æ¶æ„**: ä½¿ç”¨åŒä¸€å¥—Transformerå‚æ•°å¤„ç†å¤šæ¨¡æ€ä¿¡æ¯
3. **âš¡ é«˜æ•ˆæ¨ç†**: KVç¼“å­˜æœºåˆ¶æ˜¾è‘—åŠ é€Ÿæ–‡æœ¬ç”Ÿæˆ
4. **ğŸ¯ ç«¯åˆ°ç«¯ä¼˜åŒ–**: æ•´ä¸ªå¤šæ¨¡æ€ç®¡é“å¯ä»¥è”åˆè®­ç»ƒä¼˜åŒ–
5. **ğŸ”§ æ¶æ„æ‰©å±•æ€§**: å®Œå…¨ç»§æ‰¿CausalLMçš„é«˜çº§ç‰¹æ€§(MoEã€GQAç­‰)

è¿™ç§è®¾è®¡æ—¢ä¿æŒäº†å¼ºå¤§çš„å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ï¼Œåˆå®ç°äº†å·¥ç¨‹ä¸Šçš„é«˜æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ã€‚