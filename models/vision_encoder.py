import os
from typing import List, Optional, Tuple, Union
import torch
import torch.nn as nn
from transformers import CLIPModel, CLIPProcessor

from configs.llmconfig import llmconfig
from models.llm import CausalLM, MOEFeedForward


class VLLMconfig(llmconfig):
    """
    è§†è§‰è¯­è¨€æ¨¡å‹é…ç½®ç±» - æ‰©å±•åŸºç¡€LLMé…ç½®ä»¥æ”¯æŒè§†è§‰è¾“å…¥
    
    ç»§æ‰¿è‡ªllmconfigï¼Œæ·»åŠ è§†è§‰ç›¸å…³çš„ç‰¹æ®Šé…ç½®å‚æ•°ï¼š
    - å›¾åƒç‰¹æ®Štokençš„å®šä¹‰
    - å›¾åƒtoken IDçš„æ˜ å°„
    - è§†è§‰ç¼–ç å™¨ç›¸å…³å‚æ•°
    """
    model_type = 'vision'  # æ¨¡å‹ç±»å‹æ ‡è¯†ï¼Œç”¨äºåŒºåˆ†çº¯æ–‡æœ¬æ¨¡å‹

    def __init__(
            self,
            image_special_token: str = '@' * 196,  # å›¾åƒå ä½ç¬¦tokenï¼Œé»˜è®¤196ä¸ª@ç¬¦å·å¯¹åº”ViTçš„196ä¸ªpatch
            image_ids: List = [34] * 196,          # å›¾åƒtokenå¯¹åº”çš„IDåˆ—è¡¨ï¼Œç”¨äºåœ¨tokenåºåˆ—ä¸­æ ‡è¯†å›¾åƒä½ç½®
            **kwargs,                               # å…¶ä»–ç»§æ‰¿è‡ªllmconfigçš„å‚æ•°
    ):
        """
        åˆå§‹åŒ–è§†è§‰è¯­è¨€æ¨¡å‹é…ç½®
        
        Args:
            image_special_token: å›¾åƒåœ¨æ–‡æœ¬ä¸­çš„å ä½ç¬¦ï¼Œé•¿åº¦å¯¹åº”å›¾åƒpatchæ•°é‡
            image_ids: å›¾åƒtokençš„IDåˆ—è¡¨ï¼Œç”¨äºåœ¨è¾“å…¥åºåˆ—ä¸­è¯†åˆ«å›¾åƒä½ç½®
            **kwargs: ä¼ é€’ç»™çˆ¶ç±»llmconfigçš„å…¶ä»–é…ç½®å‚æ•°
        """
        self.image_special_token = image_special_token  # å­˜å‚¨å›¾åƒç‰¹æ®Štoken
        self.image_ids = image_ids                      # å­˜å‚¨å›¾åƒtoken IDåˆ—è¡¨
        super().__init__(**kwargs)                      # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–æ–¹æ³•

class VisionEncoder(torch.nn.Module):
    """
    è§†è§‰ç‰¹å¾æŠ•å½±å™¨ - å°†è§†è§‰ç¼–ç å™¨çš„ç‰¹å¾æ˜ å°„åˆ°è¯­è¨€æ¨¡å‹çš„éšè—ç©ºé—´
    
    ä¸»è¦åŠŸèƒ½ï¼š
    1. æ¥æ”¶æ¥è‡ªCLIPç­‰è§†è§‰ç¼–ç å™¨çš„ç‰¹å¾å‘é‡
    2. é€šè¿‡çº¿æ€§å˜æ¢å°†è§†è§‰ç‰¹å¾ç»´åº¦å¯¹é½åˆ°è¯­è¨€æ¨¡å‹çš„éšè—ç»´åº¦  
    3. ç¡®ä¿è§†è§‰å’Œæ–‡æœ¬ç‰¹å¾èƒ½å¤Ÿåœ¨åŒä¸€è¯­ä¹‰ç©ºé—´ä¸­è¿›è¡Œäº¤äº’
    """
    
    def __init__(
            self,
            ve_hidden_size: int = 512,  # è§†è§‰ç¼–ç å™¨è¾“å‡ºçš„ç‰¹å¾ç»´åº¦ï¼ˆå¦‚CLIP-ViTçš„è¾“å‡ºç»´åº¦ï¼‰
            hidden_size: int = 512,     # è¯­è¨€æ¨¡å‹çš„éšè—å±‚ç»´åº¦ï¼Œéœ€è¦ä¸LLMé…ç½®ä¸€è‡´
    ):
        """
        åˆå§‹åŒ–è§†è§‰æŠ•å½±å™¨
        
        Args:
            ve_hidden_size: è§†è§‰ç¼–ç å™¨è¾“å‡ºç‰¹å¾çš„ç»´åº¦
            hidden_size: ç›®æ ‡è¯­è¨€æ¨¡å‹éšè—å±‚çš„ç»´åº¦
        """
        super().__init__()
        self.ve_hidden_size = ve_hidden_size  # ä¿å­˜è§†è§‰ç¼–ç å™¨ç‰¹å¾ç»´åº¦
        self.hidden_size = hidden_size        # ä¿å­˜ç›®æ ‡éšè—å±‚ç»´åº¦
        
        # è§†è§‰ç‰¹å¾æŠ•å½±å±‚ - ç®€å•çš„çº¿æ€§å˜æ¢
        # å°†è§†è§‰ç‰¹å¾ä»ve_hidden_sizeç»´åº¦æ˜ å°„åˆ°hidden_sizeç»´åº¦
        self.vision_proj = nn.Sequential(
            nn.Linear(
                self.ve_hidden_size,  # è¾“å…¥ç»´åº¦ï¼šè§†è§‰ç¼–ç å™¨ç‰¹å¾ç»´åº¦
                self.hidden_size,     # è¾“å‡ºç»´åº¦ï¼šè¯­è¨€æ¨¡å‹éšè—å±‚ç»´åº¦
            )
            # æ³¨ï¼šè¿™é‡Œå¯ä»¥æ‰©å±•ä¸ºæ›´å¤æ‚çš„MLPç»“æ„ï¼Œå¦‚æ·»åŠ æ¿€æ´»å‡½æ•°ã€dropoutç­‰
            # ä¾‹å¦‚ï¼šnn.ReLU(), nn.Dropout(0.1), nn.Linear(hidden_size, hidden_size)
        )

    def forward(
            self,
            image_encoders: torch.Tensor  # è¾“å…¥çš„å›¾åƒç¼–ç ç‰¹å¾
    ) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­ï¼šå°†å›¾åƒç‰¹å¾æŠ•å½±åˆ°è¯­è¨€æ¨¡å‹ç©ºé—´
        
        Args:
            image_encoders: å›¾åƒç¼–ç å™¨è¾“å‡ºçš„ç‰¹å¾å¼ é‡
                          å½¢çŠ¶: [batch_size, num_patches, ve_hidden_size]
                          æˆ–: [num_patches, ve_hidden_size] (å•å¼ å›¾åƒ)
        
        Returns:
            vision_proj: æŠ•å½±åçš„è§†è§‰ç‰¹å¾ï¼Œç»´åº¦å¯¹é½åˆ°è¯­è¨€æ¨¡å‹
                        å½¢çŠ¶: [batch_size, num_patches, hidden_size]
                        æˆ–: [num_patches, hidden_size]
        """
        # é€šè¿‡çº¿æ€§æŠ•å½±å±‚å˜æ¢è§†è§‰ç‰¹å¾ç»´åº¦
        # è¿™æ˜¯å¤šæ¨¡æ€èåˆçš„å…³é”®æ­¥éª¤ï¼šå°†è§†è§‰ä¿¡æ¯æ˜ å°„åˆ°æ–‡æœ¬è¯­ä¹‰ç©ºé—´
        vision_proj = self.vision_proj(image_encoders)
        return vision_proj  
    
    
class VLM(CausalLM):
    """
    è§†è§‰è¯­è¨€æ¨¡å‹ - åŸºäºCausalLMçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹
    
    æ¶æ„è®¾è®¡ï¼š
    1. ç»§æ‰¿CausalLMçš„å®Œæ•´è¯­è¨€å»ºæ¨¡èƒ½åŠ›ï¼ˆåŒ…æ‹¬MoEã€GQAç­‰é«˜çº§ç‰¹æ€§ï¼‰
    2. é›†æˆCLIPè§†è§‰ç¼–ç å™¨è¿›è¡Œå›¾åƒç†è§£
    3. é€šè¿‡è§†è§‰æŠ•å½±å™¨å®ç°è§†è§‰-è¯­è¨€ç‰¹å¾èåˆ
    4. æ”¯æŒå›¾æ–‡æ··åˆè¾“å…¥çš„ç«¯åˆ°ç«¯è®­ç»ƒå’Œæ¨ç†
    
    æ ¸å¿ƒç‰¹æ€§ï¼š
    - ğŸ–¼ï¸ å¤šæ¨¡æ€è¾“å…¥ï¼šåŒæ—¶å¤„ç†æ–‡æœ¬å’Œå›¾åƒ
    - ğŸ§  ç»Ÿä¸€æ¶æ„ï¼šå…±äº«è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›
    - âš¡ é«˜æ•ˆæ¨ç†ï¼šæ”¯æŒKVç¼“å­˜åŠ é€Ÿç”Ÿæˆ
    - ğŸ¯ çµæ´»é…ç½®ï¼šæ”¯æŒä¸åŒè§„æ¨¡çš„è§†è§‰ç¼–ç å™¨
    """
    config_class = VLLMconfig  # æŒ‡å®šä½¿ç”¨çš„é…ç½®ç±»

    def __init__(self, 
                 params: VLLMconfig = None, 
                 vision_model_path: str = "./models/vision_model/clip-vit-base-patch16"):
        """
        åˆå§‹åŒ–VLLVLMæ¨¡å‹
        
        Args:
            params: VLLMconfigé…ç½®å¯¹è±¡ï¼ŒåŒ…å«æ¨¡å‹çš„æ‰€æœ‰è¶…å‚æ•°
            vision_model_path: CLIPè§†è§‰æ¨¡å‹çš„æœ¬åœ°è·¯å¾„æˆ–HuggingFaceæ¨¡å‹å
        """
        # åˆå§‹åŒ–è¯­è¨€æ¨¡å‹éƒ¨åˆ†ï¼ˆç»§æ‰¿CausalLMçš„æ‰€æœ‰åŠŸèƒ½ï¼‰
        super().__init__(params)
        
        # è®¾ç½®é»˜è®¤é…ç½®
        if not params: 
            params = VLLMconfig()
        self.params = params
        
        # åŠ è½½å¹¶åˆå§‹åŒ–è§†è§‰ç»„ä»¶
        # 1. åŠ è½½é¢„è®­ç»ƒçš„CLIPè§†è§‰ç¼–ç å™¨å’Œå¤„ç†å™¨
        self.vision_encoder, self.processor = self.__class__.get_vision_model(vision_model_path)
        
        # 2. åˆå§‹åŒ–è§†è§‰æŠ•å½±å™¨ï¼Œå°†è§†è§‰ç‰¹å¾ç»´åº¦å¯¹é½åˆ°è¯­è¨€æ¨¡å‹
        self.vision_proj = VisionEncoder(hidden_size=params.hidden_size)
        # æ³¨ï¼šè¿™é‡Œå¯ä»¥æ ¹æ®å…·ä½“çš„CLIPæ¨¡å‹è°ƒæ•´ve_hidden_sizeå‚æ•°

    @staticmethod
    def get_vision_model(model_path: str) -> Tuple[Optional[CLIPModel], Optional[CLIPProcessor]]:
        """
        åŠ è½½é¢„è®­ç»ƒçš„CLIPè§†è§‰æ¨¡å‹å’Œå¤„ç†å™¨
        
        é‡‡ç”¨å†»ç»“é¢„è®­ç»ƒæ¨¡å‹çš„ç­–ç•¥ï¼Œåªè®­ç»ƒè§†è§‰æŠ•å½±å™¨éƒ¨åˆ†ï¼š
        1. ä¿æŒCLIPè§†è§‰ç¼–ç å™¨çš„é¢„è®­ç»ƒçŸ¥è¯†
        2. å‡å°‘è®­ç»ƒå‚æ•°é‡ï¼Œæé«˜è®­ç»ƒæ•ˆç‡
        3. é¿å…è§†è§‰ç¼–ç èƒ½åŠ›åœ¨å¤šæ¨¡æ€è®­ç»ƒä¸­é€€åŒ–
        
        Args:
            model_path: CLIPæ¨¡å‹è·¯å¾„ï¼ˆæœ¬åœ°ç›®å½•æˆ–HuggingFaceæ¨¡å‹åï¼‰
            
        Returns:
            model: å†»ç»“å‚æ•°çš„CLIPæ¨¡å‹ï¼ˆè¯„ä¼°æ¨¡å¼ï¼‰
            processor: CLIPå›¾åƒé¢„å¤„ç†å™¨
            å¦‚æœæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨ï¼Œè¿”å›(None, None)
        """
        # æŠ‘åˆ¶HuggingFaceçš„è¯¦ç»†æ—¥å¿—è¾“å‡º
        from transformers import logging as hf_logging
        hf_logging.set_verbosity_error()
        
        # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
        if not os.path.exists(model_path):
            print(f"è­¦å‘Šï¼šè§†è§‰æ¨¡å‹è·¯å¾„ {model_path} ä¸å­˜åœ¨ï¼Œå°†è·³è¿‡è§†è§‰åŠŸèƒ½")
            return None, None
            
        # ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½CLIPæ¨¡å‹å’Œå¤„ç†å™¨
        model = CLIPModel.from_pretrained(model_path)
        processor = CLIPProcessor.from_pretrained(model_path)
        
        # ğŸ”’ å†»ç»“è§†è§‰ç¼–ç å™¨çš„æ‰€æœ‰å‚æ•°
        # è¿™æ˜¯å¤šæ¨¡æ€è®­ç»ƒçš„å¸¸è§ç­–ç•¥ï¼Œé¿å…ç ´åé¢„è®­ç»ƒçš„è§†è§‰è¡¨ç¤ºèƒ½åŠ›
        for param in model.parameters():
            param.requires_grad = False
            
        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼Œç¦ç”¨dropoutç­‰è®­ç»ƒç‰¹å®šçš„å±‚
        return model.eval(), processor

    @staticmethod
    def image2tensor(image, processor) -> torch.Tensor:
        """
        å°†PILå›¾åƒè½¬æ¢ä¸ºæ¨¡å‹å¯å¤„ç†çš„å¼ é‡æ ¼å¼
        
        2. åº”ç”¨CLIPé¢„å¤„ç†ï¼ˆresizeã€normalizeç­‰ï¼‰
        3. è½¬æ¢ä¸ºPyTorchå¼ é‡æ ¼å¼
        
        Args:
            image: PILå›¾åƒå¯¹è±¡
            processor: CLIPå›¾åƒå¤„ç†å™¨
            
        Returns:
            inputs: é¢„å¤„ç†åçš„å›¾åƒå¼ é‡
                   å½¢çŠ¶: [1, 3, 224, 224] (æ ‡å‡†CLIPè¾“å…¥)
        """
        # ğŸ“¸ å›¾åƒæ ¼å¼æ ‡å‡†åŒ–
        # å°†RGBAã€LAç­‰æ ¼å¼è½¬æ¢ä¸ºæ ‡å‡†çš„RGBæ ¼å¼ï¼Œç¡®ä¿å…¼å®¹æ€§
        if image.mode in ['RGBA', 'LA']: 
            image = image.convert('RGB')
            
        # ğŸ”„ åº”ç”¨CLIPé¢„å¤„ç†ç®¡é“
        # åŒ…æ‹¬ï¼šresizeåˆ°224x224ã€å½’ä¸€åŒ–ã€tensorè½¬æ¢ç­‰
        inputs = processor(images=image, return_tensors="pt")['pixel_values']
        return inputs

    @staticmethod
    def get_image_embeddings(image_tensors: torch.Tensor, vision_model: CLIPModel) -> torch.Tensor:
        """
        ä½¿ç”¨CLIPè§†è§‰ç¼–ç å™¨æå–å›¾åƒç‰¹å¾
        
        ç‰¹å¾æå–ç»†èŠ‚ï¼š
        1. ä½¿ç”¨æ— æ¢¯åº¦æ¨¡å¼ï¼Œç¡®ä¿è§†è§‰ç¼–ç å™¨å‚æ•°ä¸è¢«æ›´æ–°
        2. è·å–ViTçš„patchçº§ç‰¹å¾ï¼ˆæ’é™¤[CLS] tokenï¼‰
        3. æ¯å¼ å›¾åƒäº§ç”Ÿ196ä¸ªpatchç‰¹å¾ï¼ˆ14x14ç½‘æ ¼ï¼‰
        
        Args:
            image_tensors: é¢„å¤„ç†åçš„å›¾åƒå¼ é‡ [batch_size, 3, 224, 224]
            vision_model: å†»ç»“çš„CLIPæ¨¡å‹
            
        Returns:
            img_embedding: å›¾åƒpatchç‰¹å¾
                          å½¢çŠ¶: [196, hidden_size] æˆ– [batch_size, 196, hidden_size]
        """
        # ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼ŒèŠ‚çœå†…å­˜å¹¶ç¡®ä¿è§†è§‰ç¼–ç å™¨å‚æ•°ä¸è¢«æ›´æ–°
        with torch.no_grad():
            # é€šè¿‡CLIPè§†è§‰ç¼–ç å™¨å¤„ç†å›¾åƒ
            outputs = vision_model.vision_model(pixel_values=image_tensors)
            
        # æå–patchç‰¹å¾ï¼ˆæ’é™¤[CLS] tokenï¼‰
        # last_hidden_stateå½¢çŠ¶: [batch_size, 197, hidden_size] (1ä¸ª[CLS] + 196ä¸ªpatch)
        # [:, 1:, :] è¡¨ç¤ºè·³è¿‡ç¬¬ä¸€ä¸ª[CLS] tokenï¼Œåªä¿ç•™196ä¸ªpatchç‰¹å¾
        img_embedding = outputs.last_hidden_state[:, 1:, :].squeeze()
        
        return img_embedding

    def count_vision_proj(self, tokens: torch.Tensor, h: torch.Tensor, 
                         vision_tensors: Optional[torch.Tensor] = None, 
                         seqlen: int = 512) -> torch.Tensor:
        """
        è§†è§‰ç‰¹å¾èåˆæ ¸å¿ƒå‡½æ•° - å°†å›¾åƒç‰¹å¾åµŒå…¥åˆ°æ–‡æœ¬åºåˆ—ä¸­
        
        å®ç°å¤šæ¨¡æ€èåˆçš„å…³é”®ç®—æ³•ï¼š
        1. åœ¨tokenåºåˆ—ä¸­å®šä½å›¾åƒå ä½ç¬¦ä½ç½®
        2. å°†å¯¹åº”çš„è§†è§‰ç‰¹å¾æŠ•å½±åæ›¿æ¢å ä½ç¬¦
        3. æ„å»ºå›¾æ–‡èåˆçš„ç»Ÿä¸€è¡¨ç¤ºåºåˆ—
        
        Args:
            tokens: è¾“å…¥çš„tokenåºåˆ— [batch_size, seq_len]
            h: æ–‡æœ¬åµŒå…¥ç‰¹å¾ [batch_size, seq_len, hidden_size]
            vision_tensors: å›¾åƒç‰¹å¾å¼ é‡ [batch_size, num_images, 196, vision_hidden_size]
            seqlen: åºåˆ—æœ€å¤§é•¿åº¦é™åˆ¶
            
        Returns:
            èåˆè§†è§‰ç‰¹å¾åçš„éšè—çŠ¶æ€ [batch_size, seq_len, hidden_size]
        """
        
        def find_indices(tokens: torch.Tensor, image_ids: List[int]) -> Optional[dict]:
            """
            åœ¨tokenåºåˆ—ä¸­æŸ¥æ‰¾å›¾åƒå ä½ç¬¦çš„ä½ç½®ç´¢å¼•
            
            ä½¿ç”¨æ»‘åŠ¨çª—å£ç®—æ³•åŒ¹é…image_idsåºåˆ—ï¼š
            1. åˆ›å»ºé•¿åº¦ä¸ºlen(image_ids)çš„æ»‘åŠ¨çª—å£
            2. é€ä½ç½®æ¯”è¾ƒæ˜¯å¦å®Œå…¨åŒ¹é…image_idsæ¨¡å¼
            3. è¿”å›æ¯ä¸ªbatchä¸­æ‰€æœ‰åŒ¹é…ä½ç½®çš„èµ·æ­¢ç´¢å¼•
            
            Args:
                tokens: tokenåºåˆ— [batch_size, seq_len]
                image_ids: å›¾åƒå ä½ç¬¦IDåˆ—è¡¨
                
            Returns:
                åŒ¹é…ç»“æœå­—å…¸ {batch_idx: [(start_idx, end_idx), ...]}
                å¦‚æœæ²¡æœ‰åŒ¹é…ï¼Œè¿”å›None
            """
            # å°†image_idsè½¬æ¢ä¸ºtensorå¹¶ç§»åˆ°ç›¸åŒè®¾å¤‡
            image_ids_tensor = torch.tensor(image_ids).to(tokens.device)
            len_image_ids = len(image_ids)
            
            # åºåˆ—é•¿åº¦æ£€æŸ¥ï¼šå¦‚æœimage_idsæ¯”tokenåºåˆ—è¿˜é•¿ï¼Œæ— æ³•åŒ¹é…
            if len_image_ids > tokens.size(1):
                return None
                
            # ğŸ” æ»‘åŠ¨çª—å£åŒ¹é…ç®—æ³•
            # unfoldåˆ›å»ºæ»‘åŠ¨çª—å£è§†å›¾ï¼š[batch_size, num_windows, window_size]
            tokens_view = tokens.unfold(1, len_image_ids, 1)
            
            # é€çª—å£æ¯”è¾ƒï¼Œæ‰¾å‡ºå®Œå…¨åŒ¹é…image_idsçš„ä½ç½®
            matches = (tokens_view == image_ids_tensor).all(dim=2)
            
            # æ„å»ºåŒ¹é…ç»“æœå­—å…¸
            return {
                batch_idx: [(idx.item(), idx.item() + len_image_ids - 1) for idx in
                            matches[batch_idx].nonzero(as_tuple=True)[0]]
                for batch_idx in range(tokens.size(0)) if matches[batch_idx].any()
            } or None

        # ğŸ” å®šä½å›¾åƒå ä½ç¬¦åœ¨tokenåºåˆ—ä¸­çš„ä½ç½®
        image_indices = find_indices(tokens, self.params.image_ids)
        
        # ğŸ–¼ï¸ å¦‚æœå­˜åœ¨å›¾åƒæ•°æ®å’ŒåŒ¹é…ä½ç½®ï¼Œæ‰§è¡Œè§†è§‰ç‰¹å¾èåˆ
        if vision_tensors is not None and image_indices:
            # é€šè¿‡è§†è§‰æŠ•å½±å™¨å°†è§†è§‰ç‰¹å¾æ˜ å°„åˆ°è¯­è¨€æ¨¡å‹ç©ºé—´
            vision_proj = self.vision_proj(vision_tensors)
            
            # ç¡®ä¿è§†è§‰ç‰¹å¾æœ‰æ­£ç¡®çš„æ‰¹æ¬¡ç»´åº¦
            if len(vision_proj.shape) == 3:
                vision_proj = vision_proj.unsqueeze(0)
                
            # ğŸ”„ é€batchå¤„ç†ç‰¹å¾èåˆ
            new_h = []
            for i in range(h.size(0)):
                if i in image_indices:  # å½“å‰batchåŒ…å«å›¾åƒ
                    h_i = h[i]  # è·å–å½“å‰batchçš„æ–‡æœ¬ç‰¹å¾
                    img_idx = 0  # å›¾åƒç´¢å¼•è®¡æ•°å™¨
                    
                    # ğŸ¯ æ›¿æ¢æ¯ä¸ªå›¾åƒå ä½ç¬¦ä½ç½®
                    for start_idx, end_idx in image_indices[i]:
                        if img_idx < vision_proj.size(1):  # ç¡®ä¿ä¸è¶…å‡ºå›¾åƒæ•°é‡
                            # ğŸ§© ç‰¹å¾æ‹¼æ¥ï¼šæ–‡æœ¬å‰ç¼€ + è§†è§‰ç‰¹å¾ + æ–‡æœ¬åç¼€
                            h_i = torch.cat((
                                h_i[:start_idx],              # å›¾åƒä½ç½®ä¹‹å‰çš„æ–‡æœ¬ç‰¹å¾
                                vision_proj[i][img_idx],      # æŠ•å½±åçš„è§†è§‰ç‰¹å¾
                                h_i[end_idx + 1:]             # å›¾åƒä½ç½®ä¹‹åçš„æ–‡æœ¬ç‰¹å¾
                            ), dim=0)[:seqlen]  # æˆªæ–­åˆ°æœ€å¤§åºåˆ—é•¿åº¦
                            img_idx += 1
                    
                    new_h.append(h_i)
                else:  # å½“å‰batchä¸åŒ…å«å›¾åƒï¼Œä¿æŒåŸæ–‡æœ¬ç‰¹å¾
                    new_h.append(h[i])
                    
            # é‡æ–°å †å ä¸ºæ‰¹æ¬¡å¼ é‡
            return torch.stack(new_h, dim=0)
            
        # å¦‚æœæ²¡æœ‰å›¾åƒæ•°æ®ï¼Œç›´æ¥è¿”å›åŸå§‹æ–‡æœ¬ç‰¹å¾
        return h

    def forward(self,
                input_ids: Optional[torch.Tensor] = None,
                attention_mask: Optional[torch.Tensor] = None,
                past_key_values: Optional[List[Tuple[torch.Tensor, torch.Tensor]]] = None,
                use_cache: bool = False,
                logits_to_keep: Union[int, torch.Tensor] = 0,
                pixel_values: Optional[torch.FloatTensor] = None,
                **args):
        """
        VLLVLMå‰å‘ä¼ æ’­ - å¤šæ¨¡æ€è¯­è¨€å»ºæ¨¡çš„æ ¸å¿ƒæµç¨‹
        
        å®ç°ç«¯åˆ°ç«¯çš„è§†è§‰è¯­è¨€ç†è§£ä¸ç”Ÿæˆï¼š
        1. ğŸ“ æ–‡æœ¬ç¼–ç ï¼šå°†input_idsè½¬æ¢ä¸ºæ–‡æœ¬åµŒå…¥
        2. ğŸ–¼ï¸ è§†è§‰ç¼–ç ï¼šå¤„ç†å›¾åƒå¹¶æå–è§†è§‰ç‰¹å¾  
        3. ğŸ”— å¤šæ¨¡æ€èåˆï¼šå°†è§†è§‰ç‰¹å¾åµŒå…¥æ–‡æœ¬åºåˆ—
        4. ğŸ§  Transformerå¤„ç†ï¼šé€šè¿‡å¤šå±‚æ³¨æ„åŠ›æœºåˆ¶å»ºæ¨¡
        5. ğŸ“Š è¾“å‡ºç”Ÿæˆï¼šäº§ç”Ÿä¸‹ä¸€tokençš„æ¦‚ç‡åˆ†å¸ƒ
        
        Args:
            input_ids: è¾“å…¥tokenåºåˆ— [batch_size, seq_len]
            attention_mask: æ³¨æ„åŠ›æ©ç  [batch_size, seq_len]
            past_key_values: KVç¼“å­˜ï¼Œç”¨äºåŠ é€Ÿæ¨ç†ç”Ÿæˆ
            use_cache: æ˜¯å¦å¯ç”¨KVç¼“å­˜
            logits_to_keep: ä¿ç•™çš„logitsæ•°é‡ï¼ˆç”¨äºå†…å­˜ä¼˜åŒ–ï¼‰
            pixel_values: å›¾åƒåƒç´ å€¼ [batch_size, num_images, channels, height, width]
            
        Returns:
            åŒ…å«logitsã€hidden_statesã€aux_lossç­‰çš„è¾“å‡ºå¯¹è±¡
        """
        # è·å–è¾“å…¥ç»´åº¦ä¿¡æ¯
        batch_size, seq_length = input_ids.shape
        
        # åˆå§‹åŒ–æˆ–è·å–KVç¼“å­˜
        past_key_values = past_key_values or [None] * len(self.model.layers)
        
        # è®¡ç®—å½“å‰ç”Ÿæˆä½ç½®ï¼ˆç”¨äºä½ç½®ç¼–ç å’ŒKVç¼“å­˜ï¼‰
        start_pos = past_key_values[0][0].shape[1] if past_key_values[0] is not None else 0

        # Step 1: æ–‡æœ¬åµŒå…¥
        # å°†token IDsè½¬æ¢ä¸ºå¯†é›†çš„å‘é‡è¡¨ç¤º
        hidden_states = self.model.dropout(self.model.embed_tokens(input_ids))

        # Step 2 & 3: è§†è§‰å¤„ç†ä¸å¤šæ¨¡æ€èåˆ
        # åªåœ¨åˆå§‹è¾“å…¥æ—¶å¤„ç†å›¾åƒï¼ˆstart_pos == 0ï¼‰ï¼Œç”Ÿæˆé˜¶æ®µè·³è¿‡
        if pixel_values is not None and start_pos == 0:
            # å¤„ç†å›¾åƒå¼ é‡ç»´åº¦
            # å¦‚æœè¾“å…¥æ˜¯6ç»´å¼ é‡ï¼Œå»æ‰å¤šä½™çš„ç»´åº¦
            if len(pixel_values.shape) == 6:
                pixel_values = pixel_values.squeeze(2)
            
            # è§£æå›¾åƒå¼ é‡ç»´åº¦ï¼š[batch_size, num_images, channels, height, width]
            bs, num, c, im_h, im_w = pixel_values.shape
            
            # ğŸ¯ æ‰¹é‡å¤„ç†å›¾åƒç‰¹å¾æå–
            # æ ¹æ®æ‰¹æ¬¡å¤§å°å†³å®šå †å ç»´åº¦
            stack_dim = 1 if bs > 1 else 0
            
            # ğŸ”„ é€å¼ å›¾åƒæå–ç‰¹å¾å¹¶å †å 
            vision_tensors = torch.stack([
                VLM.get_image_embeddings(
                    pixel_values[:, i, :, :, :],  # ç¬¬iå¼ å›¾åƒ
                    self.vision_encoder             # CLIPè§†è§‰ç¼–ç å™¨
                )
                for i in range(num)  # éå†æ‰€æœ‰å›¾åƒ
            ], dim=stack_dim)
            
            # ğŸ”— æ‰§è¡Œè§†è§‰-æ–‡æœ¬ç‰¹å¾èåˆ
            # å°†æŠ•å½±åçš„è§†è§‰ç‰¹å¾æ›¿æ¢æ–‡æœ¬åºåˆ—ä¸­çš„å›¾åƒå ä½ç¬¦
            hidden_states = self.count_vision_proj(
                tokens=input_ids, 
                h=hidden_states, 
                vision_tensors=vision_tensors,
                seqlen=input_ids.shape[1]
            )

        # ğŸ¯ Step 4: ä½ç½®ç¼–ç å‡†å¤‡
        # è·å–å¯¹åº”å½“å‰åºåˆ—ä½ç½®çš„RoPEç¼–ç 
        position_embeddings = (
            self.model.freqs_cos[start_pos:start_pos + seq_length],  # ä½™å¼¦ä½ç½®ç¼–ç 
            self.model.freqs_sin[start_pos:start_pos + seq_length]   # æ­£å¼¦ä½ç½®ç¼–ç 
        )

        # ğŸ—ï¸ Step 5: Transformerå±‚çº§è”å¤„ç†
        presents = []  # å­˜å‚¨æ–°çš„KVç¼“å­˜
        for layer_idx, (layer, past_key_value) in enumerate(zip(self.model.layers, past_key_values)):
            # ğŸ§  é€šè¿‡Transformerå±‚å¤„ç†ï¼ˆåŒ…æ‹¬æ³¨æ„åŠ›æœºåˆ¶ã€FFN/MoEç­‰ï¼‰
            hidden_states, present = layer(
                hidden_states,         # è¾“å…¥éšè—çŠ¶æ€
                position_embeddings,   # RoPEä½ç½®ç¼–ç 
                past_key_value=past_key_value,  # å†å²KVç¼“å­˜
                use_cache=use_cache,    # æ˜¯å¦ç”Ÿæˆæ–°çš„KVç¼“å­˜
                attention_mask=attention_mask   # æ³¨æ„åŠ›æ©ç 
            )
            presents.append(present)  # æ”¶é›†KVç¼“å­˜

        # ğŸ”§ Step 6: æœ€ç»ˆå±‚å½’ä¸€åŒ–
        hidden_states = self.model.norm(hidden_states)

        # ğŸ“Š è®¡ç®—MoEè¾…åŠ©æŸå¤±ï¼ˆå¦‚æœä½¿ç”¨MoEæ¶æ„ï¼‰
        aux_loss = sum(
            layer.mlp.aux_loss  # æ¯å±‚MoEçš„è´Ÿè½½å‡è¡¡æŸå¤±
            for layer in self.model.layers
            if isinstance(layer.mlp, MOEFeedForward)  # åªç»Ÿè®¡MoEå±‚
        )
        
        # ğŸ¯ Step 7: è¯­è¨€å»ºæ¨¡å¤´è¾“å‡º
        # æ ¹æ®logits_to_keepå‚æ•°å†³å®šè¾“å‡ºèŒƒå›´ï¼ˆå†…å­˜ä¼˜åŒ–ï¼‰
        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep
        logits = self.lm_head(hidden_states[:, slice_indices, :])
        
        # ğŸ“¦ æ„å»ºè¾“å‡ºå¯¹è±¡
        self.OUT.__setitem__('last_hidden_state', hidden_states)  # æœ€ç»ˆéšè—çŠ¶æ€
        self.OUT.__setitem__('logits', logits)                   # è¯æ±‡è¡¨ä¸Šçš„æ¦‚ç‡åˆ†å¸ƒ
        self.OUT.__setitem__('aux_loss', aux_loss)               # MoEè¾…åŠ©æŸå¤±
        self.OUT.__setitem__('past_key_values', presents)        # æ›´æ–°çš„KVç¼“å­˜
        
        return self.OUT