#!/usr/bin/env python3
"""
å‡†å¤‡tokenizerè®­ç»ƒæ•°æ®
åˆ›å»ºç¤ºä¾‹æ•°æ®é›†ç”¨äºè®­ç»ƒæ”¯æŒå›¾åƒå ä½ç¬¦çš„tokenizer
"""

import json
import random
from pathlib import Path
from typing import List


def create_sample_dataset(output_path: str, num_samples: int = 10000):
    """
    åˆ›å»ºç¤ºä¾‹è®­ç»ƒæ•°æ®é›†
    
    Args:
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        num_samples: ç”Ÿæˆæ ·æœ¬æ•°é‡
    """
    
    # åŸºç¡€æ–‡æœ¬æ¨¡æ¿
    text_templates = [
        # ä¸­æ–‡æ¨¡æ¿
        "è¿™æ˜¯ä¸€æ®µæ™®é€šçš„æ–‡æœ¬ã€‚",
        "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦åˆ†æ”¯ã€‚",
        "æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å›¾åƒè¯†åˆ«ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚",
        "è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯æ­£åœ¨å¿«é€Ÿå‘å±•ã€‚",
        "è®¡ç®—æœºè§†è§‰å¯ä»¥å¸®åŠ©æˆ‘ä»¬ç†è§£å›¾åƒå†…å®¹ã€‚",
        "å¤šæ¨¡æ€å­¦ä¹ ç»“åˆäº†æ–‡æœ¬å’Œå›¾åƒä¿¡æ¯ã€‚",
        "Transformeræ¶æ„é©å‘½æ€§åœ°æ”¹å˜äº†NLPé¢†åŸŸã€‚",
        "é¢„è®­ç»ƒæ¨¡å‹ä¸ºä¸‹æ¸¸ä»»åŠ¡æä¾›äº†å¼ºå¤§çš„åŸºç¡€ã€‚",
        
        # è‹±æ–‡æ¨¡æ¿
        "This is a sample text for tokenizer training.",
        "Machine learning algorithms are powerful tools.",
        "Deep neural networks can learn complex patterns.",
        "Natural language processing enables human-computer interaction.",
        "Computer vision helps machines understand visual content.",
        "Multimodal learning combines different types of data.",
        "Large language models show impressive capabilities.",
        "Pre-trained models serve as foundation for many tasks.",
        
        # æŠ€æœ¯ç›¸å…³
        "æ•°æ®é¢„å¤„ç†æ˜¯æœºå™¨å­¦ä¹ pipelineçš„é‡è¦æ­¥éª¤ã€‚",
        "ç‰¹å¾å·¥ç¨‹å¯ä»¥æ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½ã€‚",
        "æ¨¡å‹è¯„ä¼°éœ€è¦ä½¿ç”¨åˆé€‚çš„æŒ‡æ ‡ã€‚",
        "è¶…å‚æ•°è°ƒä¼˜æ˜¯æ¨¡å‹ä¼˜åŒ–çš„å…³é”®ç¯èŠ‚ã€‚",
        "æ•°æ®å¢å¼ºæŠ€æœ¯å¯ä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚",
        "æ­£åˆ™åŒ–æ–¹æ³•æœ‰åŠ©äºé˜²æ­¢è¿‡æ‹Ÿåˆã€‚",
        "é›†æˆå­¦ä¹ é€šè¿‡ç»„åˆå¤šä¸ªæ¨¡å‹æ¥æå‡æ•ˆæœã€‚",
        "è¿ç§»å­¦ä¹ åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„çŸ¥è¯†ã€‚",
    ]
    
    # å›¾åƒç›¸å…³æ¨¡æ¿
    image_templates = [
        # ä¸­æ–‡å›¾åƒæè¿°
        "è¯·çœ‹è¿™å¼ å›¾ç‰‡ï¼š{image}",
        "è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†ï¼š{image}",
        "å›¾ç‰‡å†…å®¹å¦‚ä¸‹ï¼š{image}",
        "å¦‚å›¾æ‰€ç¤ºï¼š{image}",
        "æ ¹æ®å›¾ç‰‡ {image} å¯ä»¥çœ‹å‡º",
        "å›¾åƒ {image} æ˜¾ç¤ºäº†é‡è¦ä¿¡æ¯",
        "å‚è€ƒå›¾ç‰‡ï¼š{image}",
        "ä¸‹é¢æ˜¯ç›¸å…³å›¾ç‰‡ï¼š{image}",
        
        # è‹±æ–‡å›¾åƒæè¿°  
        "Here is the image: {image}",
        "Look at this picture: {image}",
        "The image shows: {image}",
        "Based on the image {image}",
        "Referring to image {image}",
        "This picture {image} demonstrates",
        "See the following image: {image}",
        "The visual content: {image}",
        
        # å¯¹è¯å¼
        "ç”¨æˆ·ï¼šå±•ç¤ºå›¾ç‰‡ {image}",
        "ç”¨æˆ·ï¼šè¯·åˆ†æè¿™å¼ å›¾ {image}",
        "åŠ©æ‰‹ï¼šæ ¹æ®å›¾ç‰‡ {image}ï¼Œæˆ‘å¯ä»¥çœ‹åˆ°",
        "User: Show me the image {image}",
        "User: Analyze this picture {image}",
        "Assistant: Based on the image {image}, I can see",
        
        # æŒ‡ä»¤å¼
        "æè¿°å›¾ç‰‡ï¼š{image}",
        "åˆ†æå›¾åƒå†…å®¹ï¼š{image}",
        "è¯†åˆ«å›¾ç‰‡ä¸­çš„å¯¹è±¡ï¼š{image}",
        "Describe the image: {image}",
        "Analyze the visual content: {image}",
        "Identify objects in {image}",
    ]
    
    # å¤šå›¾åƒæ¨¡æ¿
    multi_image_templates = [
        "æ¯”è¾ƒè¿™ä¸¤å¼ å›¾ç‰‡ï¼š{image1} å’Œ {image2}",
        "ç¬¬ä¸€å¼ å›¾ {image1}ï¼Œç¬¬äºŒå¼ å›¾ {image2}",
        "å›¾ç‰‡åºåˆ—ï¼š{image1} {image2} {image3}",
        "Compare these images: {image1} and {image2}",
        "First image {image1}, second image {image2}",
        "Image sequence: {image1} {image2} {image3}",
    ]
    
    def generate_image_placeholder(length: int = None) -> str:
        """ç”ŸæˆæŒ‡å®šé•¿åº¦çš„å›¾åƒå ä½ç¬¦"""
        if length is None:
            # å¸¸è§çš„å›¾åƒtokené•¿åº¦
            lengths = [1, 64, 196, 256, 384, 576]
            length = random.choice(lengths)
        return "@" * length
    
    samples = []
    
    # ç”Ÿæˆçº¯æ–‡æœ¬æ ·æœ¬ (30%)
    for _ in range(int(num_samples * 0.3)):
        text = random.choice(text_templates)
        samples.append(text)
    
    # ç”Ÿæˆå•å›¾åƒæ ·æœ¬ (50%)
    for _ in range(int(num_samples * 0.5)):
        template = random.choice(image_templates)
        image_placeholder = generate_image_placeholder()
        text = template.format(image=image_placeholder)
        samples.append(text)
    
    # ç”Ÿæˆå¤šå›¾åƒæ ·æœ¬ (20%)
    for _ in range(int(num_samples * 0.2)):
        template = random.choice(multi_image_templates)
        placeholders = {
            'image1': generate_image_placeholder(),
            'image2': generate_image_placeholder(), 
            'image3': generate_image_placeholder(),
        }
        text = template.format(**placeholders)
        samples.append(text)
    
    # éšæœºæ‰“ä¹±
    random.shuffle(samples)
    
    # ä¿å­˜ä¸ºæ–‡æœ¬æ–‡ä»¶
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        for sample in samples:
            f.write(sample + '\n')
    
    print(f"âœ… å·²ç”Ÿæˆ {len(samples)} æ¡è®­ç»ƒæ ·æœ¬åˆ° {output_path}")
    
    # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
    stats = {
        'total_samples': len(samples),
        'avg_length': sum(len(s) for s in samples) / len(samples),
        'samples_with_images': sum(1 for s in samples if '@' in s),
        'total_at_symbols': sum(s.count('@') for s in samples),
    }
    
    stats_path = output_path.with_suffix('.stats.json')
    with open(stats_path, 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡å·²ä¿å­˜åˆ° {stats_path}")
    print(f"   - æ€»æ ·æœ¬æ•°: {stats['total_samples']}")
    print(f"   - å¹³å‡é•¿åº¦: {stats['avg_length']:.1f} å­—ç¬¦")
    print(f"   - åŒ…å«å›¾åƒçš„æ ·æœ¬: {stats['samples_with_images']}")
    print(f"   - æ€»@ç¬¦å·æ•°: {stats['total_at_symbols']}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ç”Ÿæˆtokenizerè®­ç»ƒæ•°æ®")
    parser.add_argument("--output", default="data/tokenizer_train.txt",
                       help="è¾“å‡ºæ–‡ä»¶è·¯å¾„ (é»˜è®¤: data/tokenizer_train.txt)")
    parser.add_argument("--num-samples", type=int, default=10000,
                       help="ç”Ÿæˆæ ·æœ¬æ•°é‡ (é»˜è®¤: 10000)")
    
    args = parser.parse_args()
    
    print("="*60)
    print("å‡†å¤‡Tokenizerè®­ç»ƒæ•°æ®")
    print("="*60)
    
    create_sample_dataset(args.output, args.num_samples)
    
    print(f"\nä½¿ç”¨æ–¹æ³•:")
    print(f"python train_tokenizer.py --data {args.output} --output tokenizers/my_tokenizer")


if __name__ == "__main__":
    main()