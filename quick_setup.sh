#!/bin/bash
# zerollm-v å¿«é€Ÿé…ç½®è„šæœ¬ (åœ¨ç°æœ‰é¡¹ç›®ä¸­è¿è¡Œ)
# é€‚ç”¨äºå·²å…‹éš†é¡¹ç›®ï¼Œåªéœ€ä¸‹è½½æ•°æ®å’Œæ¨¡å‹

set -Eeuo pipefail
IFS=$'\n\t'

# é¢œè‰²å®šä¹‰
GREEN='\033[0;32m'
BLUE='\033[0;34m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m'

log_info() { echo -e "${BLUE}[INFO]${NC} $1"; }
log_success() { echo -e "${GREEN}[SUCCESS]${NC} $1"; }
log_error() { echo -e "${RED}[ERROR]${NC} $1"; }
log_step() { echo -e "\n${YELLOW}=== $1 ===${NC}"; }

DRY_RUN="${DRY_RUN:-}"

# ä¸‹è½½æ–‡ä»¶å‡½æ•°
download_with_progress() {
    local url="$1"
    local output="$2"
    local name="$3"
    
    if [ -f "$output" ]; then
        log_info "$name å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½"
        return 0
    fi
    
    log_info "ä¸‹è½½ $name..."
    if [ -n "$DRY_RUN" ]; then
        log_info "[DRY_RUN] å°†ä¸‹è½½: $url -> $output"
        mkdir -p "$(dirname "$output")"
        : > "$output" 2>/dev/null || true
        return 0
    fi
    mkdir -p "$(dirname "$output")"
    
    if curl -L --progress-bar "$url" -o "$output"; then
        log_success "$name ä¸‹è½½å®Œæˆ"
    else
        log_error "$name ä¸‹è½½å¤±è´¥"
        return 1
    fi
}

echo -e "${BLUE}ğŸš€ zerollm-v å¿«é€Ÿé…ç½®è„šæœ¬${NC}\n"

# 1. åˆ›å»ºå¿…è¦ç›®å½•ï¼ˆä¸è®­ç»ƒè„šæœ¬/é…ç½®ä¸€è‡´ï¼‰
log_step "åˆ›å»ºç›®å½•ç»“æ„"
mkdir -p dataset model/vision_model/clip-vit-base-patch16
log_success "ç›®å½•åˆ›å»ºå®Œæˆ"

# 2. ä¸‹è½½è®­ç»ƒæ•°æ®
log_step "ä¸‹è½½è®­ç»ƒæ•°æ®"
download_with_progress \
    "https://www.modelscope.cn/datasets/gongjy/minimind-v_dataset/resolve/master/pretrain_vlm_data.jsonl" \
    "dataset/pretrain_data.jsonl" \
    "è®­ç»ƒæ•°æ®"

download_with_progress \
    "https://www.modelscope.cn/datasets/gongjy/minimind-v_dataset/resolve/master/pretrain_images.zip" \
    "dataset/pretrain_images.zip" \
    "å›¾åƒå‹ç¼©åŒ…"

# è§£å‹å›¾åƒæ•°æ®
if [ -f "dataset/pretrain_images.zip" ] && [ ! -d "dataset/pretrain_images" ]; then
    if [ -n "$DRY_RUN" ]; then
        log_info "[DRY_RUN] å°†è§£å‹: dataset/pretrain_images.zip -> dataset/pretrain_images/"
    else
        log_info "è§£å‹å›¾åƒæ•°æ®..."
        (cd dataset && unzip -q pretrain_images.zip)
        log_success "å›¾åƒæ•°æ®è§£å‹å®Œæˆ"
    fi
fi

# 3. ä¸‹è½½CLIPæ¨¡å‹
log_step "ä¸‹è½½CLIPè§†è§‰æ¨¡å‹"
base_url="https://huggingface.co/openai/clip-vit-base-patch16/resolve/main"
model_dir="model/vision_model/clip-vit-base-patch16"

files=("config.json" "preprocessor_config.json" "pytorch_model.bin" \
       "tokenizer_config.json" "tokenizer.json" "vocab.json" \
       "merges.txt" "special_tokens_map.json")

for file in "${files[@]}"; do
    download_with_progress "$base_url/$file" "$model_dir/$file" "CLIP: $file"
done

# 4. é…ç½®ç¯å¢ƒ
log_step "é…ç½®ç¯å¢ƒ"
if [ -n "${SWANLAB_API_KEY:-}" ]; then
  export SWANLAB_API_KEY="$SWANLAB_API_KEY"
  log_info "å·²è®¾ç½® SWANLAB_API_KEYï¼ˆæ¥è‡ªç¯å¢ƒå˜é‡ï¼‰"
else
  log_info "æœªæä¾› SWANLAB_API_KEYï¼ˆå¯é€‰ï¼‰"
fi
if [ -n "$DRY_RUN" ]; then
  log_info "[DRY_RUN] å°†å†™å…¥ .env å’Œ PYTHONPATH"
fi
{
  [ -n "${SWANLAB_API_KEY:-}" ] && echo "SWANLAB_API_KEY=$SWANLAB_API_KEY"
  echo "PYTHONPATH=\${PYTHONPATH}:$(pwd)/src"
} > .env
log_success "ç¯å¢ƒå˜é‡é…ç½®å®Œæˆ"

# 5. éªŒè¯æ–‡ä»¶
log_step "éªŒè¯æ–‡ä»¶å®Œæ•´æ€§"
required_files=(
    "dataset/pretrain_data.jsonl"
    "dataset/pretrain_images"
    "model/vision_model/clip-vit-base-patch16/pytorch_model.bin"
    "configs/vlm_training.yaml"
)

all_good=true
for file in "${required_files[@]}"; do
    if [ -e "$file" ]; then
        log_success "âœ“ $file"
    else
        log_error "âœ— $file ç¼ºå¤±"
        all_good=false
    fi
done

if [ "$all_good" = true ]; then
    echo -e "\n${GREEN}ğŸ‰ é…ç½®å®Œæˆï¼ç°åœ¨å¯ä»¥å¼€å§‹è®­ç»ƒï¼š${NC}"
    echo -e "${BLUE}source .env && python src/train_vlm.py configs/vlm_training.yaml${NC}\n"
else
    echo -e "\n${RED}âŒ é…ç½®ä¸å®Œæ•´ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯${NC}\n"
fi
