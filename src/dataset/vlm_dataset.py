"""
VLMè®­ç»ƒæ•°æ®é›† - æ”¯æŒJSONLæ ¼å¼çš„å¤šæ¨¡æ€å¯¹è¯æ•°æ®
é€‚é…MiniMind-Vè®­ç»ƒå™¨æ¶æ„
"""

import json
import torch
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from torch.utils.data import Dataset
from PIL import Image
import torchvision.transforms as transforms
from transformers import AutoTokenizer, GPT2Tokenizer


class VLMDataset(Dataset):
    """å¤šæ¨¡æ€å¯¹è¯æ•°æ®é›†ç±»"""
    
    def __init__(
        self, 
        data_path: str, 
        images_path: str, 
        tokenizer,
        preprocess=None,
        image_special_token="<image>",
        max_length: int = 640
    ):
        self.data_path = Path(data_path)
        self.images_path = Path(images_path)
        self.tokenizer = tokenizer
        self.preprocess = preprocess
        self.image_special_token = image_special_token
        self.max_length = max_length
        
        # é»˜è®¤å›¾åƒé¢„å¤„ç†
        if self.preprocess is None:
            self.preprocess = transforms.Compose([
                transforms.Resize((224, 224)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                                   std=[0.229, 0.224, 0.225])
            ])
        
        # åŠ è½½æ•°æ®
        self.samples = self._load_jsonl_data()
        print(f"âœ… åŠ è½½äº† {len(self.samples)} ä¸ªè®­ç»ƒæ ·æœ¬")
    
    def _load_jsonl_data(self) -> List[Dict]:
        """åŠ è½½JSONLæ ¼å¼çš„æ•°æ®"""
        samples = []
        
        if not self.data_path.exists():
            print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.data_path}")
            return samples
        
        with open(self.data_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                try:
                    line = line.strip()
                    if not line:
                        continue
                    
                    sample = json.loads(line)
                    
                    # éªŒè¯æ•°æ®æ ¼å¼
                    if not self._validate_sample(sample):
                        print(f"è·³è¿‡ç¬¬ {line_num} è¡Œ: æ•°æ®æ ¼å¼ä¸ç¬¦åˆè¦æ±‚")
                        continue
                    
                    samples.append(sample)
                    
                except json.JSONDecodeError as e:
                    print(f"ç¬¬ {line_num} è¡ŒJSONè§£æé”™è¯¯: {e}")
                    continue
                except Exception as e:
                    print(f"ç¬¬ {line_num} è¡Œå¤„ç†é”™è¯¯: {e}")
                    continue
        
        return samples
    
    def _validate_sample(self, sample: Dict) -> bool:
        """éªŒè¯æ ·æœ¬æ ¼å¼"""
        required_fields = ["conversations", "image"]
        if not all(field in sample for field in required_fields):
            return False
        
        conversations = sample["conversations"]
        if not isinstance(conversations, list) or len(conversations) == 0:
            return False
            
        for conv in conversations:
            if not isinstance(conv, dict):
                return False
            if not all(key in conv for key in ["role", "content"]):
                return False
            if conv["role"] not in ["user", "assistant"]:
                return False
                
        return True
    
    def _process_conversations(self, conversations: List[Dict]) -> str:
        """å°†å¯¹è¯è½¬æ¢ä¸ºæ–‡æœ¬"""
        text = ""
        
        for conv in conversations:
            role = conv["role"]
            content = conv["content"]
            
            # å¤„ç†å›¾åƒæ ‡è®°
            if "<image>" in content:
                content = content.replace("<image>", self.image_special_token)
            
            # æ·»åŠ è§’è‰²æ ‡è®°
            if role == "user":
                text += f"<|user|>\n{content}\n"
            elif role == "assistant":
                text += f"<|assistant|>\n{content}\n"
        
        text += "<|endoftext|>"
        return text
    
    def _load_image(self, image_name: str) -> torch.Tensor:
        """åŠ è½½å’Œé¢„å¤„ç†å›¾åƒ"""
        image_path = self.images_path / image_name
        
        if not image_path.exists():
            print(f"è­¦å‘Š: å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨ {image_path}")
            return torch.zeros(3, 224, 224)
        
        try:
            image = Image.open(image_path).convert('RGB')
            return self.preprocess(image)
        except Exception as e:
            print(f"å›¾åƒåŠ è½½é”™è¯¯ {image_path}: {e}")
            return torch.zeros(3, 224, 224)
    
    def __len__(self) -> int:
        return len(self.samples)
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        è¿”å›æ ¼å¼: (input_ids, labels, loss_mask, pixel_values)
        """
        sample = self.samples[idx]
        
        # å¤„ç†æ–‡æœ¬
        text = self._process_conversations(sample["conversations"])
        
        # Tokenization
        encoding = self.tokenizer(
            text,
            max_length=self.max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )
        
        input_ids = encoding["input_ids"].squeeze(0)
        attention_mask = encoding["attention_mask"].squeeze(0)
        
        # åˆ›å»ºlabels (ç”¨äºè¯­è¨€å»ºæ¨¡)
        labels = input_ids.clone()
        labels[:-1] = input_ids[1:]  # å‘å·¦ç§»åŠ¨ä¸€ä½
        labels[-1] = -100  # æœ€åä¸€ä¸ªtokenä¸å‚ä¸æŸå¤±è®¡ç®—
        
        # åˆ›å»ºloss_mask
        loss_mask = attention_mask.clone().float()
        
        # åŠ è½½å›¾åƒ
        pixel_values = self._load_image(sample["image"])
        
        return input_ids, labels, loss_mask, pixel_values


def create_vlm_dataset(
    data_path: str, 
    images_path: str, 
    tokenizer_path: str = None,
    tokenizer = None,
    max_length: int = 640,
    image_special_token: str = "<image>"
) -> VLMDataset:
    """
    åˆ›å»ºVLMæ•°æ®é›†
    
    Args:
        data_path: JSONLæ•°æ®æ–‡ä»¶è·¯å¾„
        images_path: å›¾åƒæ–‡ä»¶å¤¹è·¯å¾„
        tokenizer_path: tokenizerè·¯å¾„ (å¯é€‰)
        tokenizer: tokenizerå¯¹è±¡ (å¯é€‰ï¼Œä¼˜å…ˆä½¿ç”¨)
        max_length: æœ€å¤§åºåˆ—é•¿åº¦
        image_special_token: å›¾åƒç‰¹æ®Štoken
        
    Returns:
        VLMDatasetå®ä¾‹
    """
    # åŠ è½½æˆ–ä½¿ç”¨æä¾›çš„tokenizer
    if tokenizer is None:
        if tokenizer_path:
            try:
                tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, use_fast=True)
                print(f"âœ… åŠ è½½è‡ªå®šä¹‰tokenizer: {tokenizer_path}")
            except Exception as e:
                print(f"âŒ åŠ è½½è‡ªå®šä¹‰tokenizerå¤±è´¥: {e}")
                tokenizer = None
        
        if tokenizer is None:
            print("ğŸ”„ ä½¿ç”¨GPT2 tokenizerä½œä¸ºå¤‡é€‰")
            tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
            special_tokens = {
                "pad_token": "<pad>",
                "additional_special_tokens": ["<|user|>", "<|assistant|>", "<|endoftext|>", image_special_token]
            }
            tokenizer.add_special_tokens(special_tokens)
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = VLMDataset(
        data_path=data_path,
        images_path=images_path,
        tokenizer=tokenizer,
        max_length=max_length,
        image_special_token=image_special_token
    )
    
    return dataset